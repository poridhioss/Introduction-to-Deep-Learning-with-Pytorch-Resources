{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Building Custom Datasets\n",
    "\n",
    "In this lab, we will build a custom Dataset class from scratch to understand how PyTorch loads data.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Understand when and why to create custom Dataset classes\n",
    "- Implement the three essential methods: `__init__`, `__len__`, `__getitem__`\n",
    "- Build a dataset that replicates `ImageFolder` functionality\n",
    "- Compare custom datasets with built-in alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('data/')\n",
    "image_path = data_path / 'pizza_steak_sushi'\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} directory exists.')\n",
    "else:\n",
    "    print(f'Creating {image_path} directory...')\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    with open(data_path / 'pizza_steak_sushi.zip', 'wb') as f:\n",
    "        request = requests.get('https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip')\n",
    "        print('Downloading data...')\n",
    "        f.write(request.content)\n",
    "    with zipfile.ZipFile(data_path / 'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
    "        print('Unzipping data...')\n",
    "        zip_ref.extractall(image_path)\n",
    "    print('Done!')\n",
    "\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "print(f'Train dir: {train_dir}')\n",
    "print(f'Test dir: {test_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Helper Function: Find Classes\n",
    "\n",
    "Before building our custom dataset, we need a helper function to discover classes from the directory structure.\n",
    "\n",
    "### What This Function Does:\n",
    "\n",
    "1. **Scans a directory** for subdirectories (each subdirectory = one class)\n",
    "2. **Creates a mapping** from class names to integer indices\n",
    "3. **Returns both** the class list and the mapping dictionary\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "We'll use `os.scandir()` to traverse the target directory (which should be in standard image classification format):\n",
    "- Get the class names by finding all subdirectories\n",
    "- Raise an error if no class folders are found\n",
    "- Turn the class names into a dictionary of numerical labels\n",
    "\n",
    "**Important:** This mimics what `ImageFolder` does internally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    \n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f'No class folders found in {directory}')\n",
    "    \n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "classes, class_to_idx = find_classes(train_dir)\n",
    "print(f'Classes: {classes}')\n",
    "print(f'Class to idx: {class_to_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Works Correctly! ‚úÖ\n",
    "\n",
    "Our `find_classes()` function successfully:\n",
    "- Found all 3 classes in alphabetical order\n",
    "- Created the correct index mapping (pizza‚Üí0, steak‚Üí1, sushi‚Üí2)\n",
    "- Matches the behavior of PyTorch's `ImageFolder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Custom Dataset Class\n",
    "\n",
    "Now we're ready to build our own custom Dataset.\n",
    "\n",
    "### Why Build Custom Datasets?\n",
    "\n",
    "While `ImageFolder` works for standard image classification, you might need custom datasets when:\n",
    "- Your data has a **non-standard format** (CSV annotations, JSON metadata)\n",
    "- You need **custom preprocessing** or **complex augmentations**\n",
    "- You're working with **multiple data sources** (images + text + tabular)\n",
    "- Your labels are stored **separately** from images\n",
    "\n",
    "### The Three Essential Methods\n",
    "\n",
    "Every PyTorch Dataset must implement:\n",
    "\n",
    "| Method | Purpose |\n",
    "|--------|---------|\n",
    "| `__init__(self, ...)` | Initialize paths, transforms, load metadata |\n",
    "| `__len__(self)` | Return total number of samples |\n",
    "| `__getitem__(self, idx)` | Return one sample (image, label) given an index |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderCustom(Dataset):\n",
    "    \n",
    "    def __init__(self, targ_dir: str, transform=None) -> None:\n",
    "        self.paths = list(pathlib.Path(targ_dir).glob('*/*.jpg'))\n",
    "        self.transform = transform\n",
    "        self.classes, self.class_to_idx = find_classes(targ_dir)\n",
    "    \n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        img = self.load_image(index)\n",
    "        class_name = self.paths[index].parent.name\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "        \n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx\n",
    "        else:\n",
    "            return img, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Implementation\n",
    "\n",
    "Let's break down each part of our custom dataset:\n",
    "\n",
    "**1. Subclass `torch.utils.data.Dataset`**\n",
    "- Inherit from PyTorch's base Dataset class\n",
    "\n",
    "**2. `__init__` Method**\n",
    "- `targ_dir`: Target directory containing the images\n",
    "- `transform`: Optional transforms to apply to images\n",
    "- `self.paths`: List of all image file paths using `pathlib.Path.glob()`\n",
    "- `self.classes` & `self.class_to_idx`: From our `find_classes()` function\n",
    "\n",
    "**3. `load_image` Method**\n",
    "- Helper method to load images from file using PIL\n",
    "- Returns a PIL Image object\n",
    "- Separated for clarity and potential customization\n",
    "\n",
    "**4. `__len__` Method (Required)**\n",
    "- Overrides the Dataset's `__len__` method\n",
    "- Returns the number of samples in the dataset\n",
    "- Allows calling `len(dataset)`\n",
    "\n",
    "**5. `__getitem__` Method (Required)**\n",
    "- Overrides the Dataset's `__getitem__` method\n",
    "- Loads the image at the given index\n",
    "- Extracts class name from the file path\n",
    "- Converts class name to integer index\n",
    "- Applies transforms if provided\n",
    "- Returns tuple: `(transformed_image, class_index)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Custom Datasets\n",
    "\n",
    "Now let's instantiate our custom dataset class with proper transforms.\n",
    "\n",
    "### Defining Transforms\n",
    "\n",
    "Before testing our `ImageFolderCustom` class, we need to create transforms to prepare our images.\n",
    "\n",
    "**Critical Distinction:**\n",
    "\n",
    "| Transform Set | Augmentation | Purpose |\n",
    "|---------------|--------------|---------|\n",
    "| **Training** | ‚úÖ `RandomHorizontalFlip(p=0.5)` | Improve generalization |\n",
    "| **Testing** | ‚ùå No augmentation | Consistent, fair evaluation |\n",
    "\n",
    "**Why no augmentation for testing?**\n",
    "- Test set should represent real-world conditions\n",
    "- Augmentation adds randomness that makes evaluation inconsistent\n",
    "- We want reproducible results for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Datasets\n",
    "\n",
    "Now let's create `Dataset` objects for both training and testing images using our `ImageFolderCustom` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_custom = ImageFolderCustom(\n",
    "    targ_dir=train_dir,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "test_data_custom = ImageFolderCustom(\n",
    "    targ_dir=test_dir,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_data_custom)}')\n",
    "print(f'Test samples: {len(test_data_custom)}')\n",
    "print(f'Classes: {train_data_custom.classes}')\n",
    "print(f'Class_to_idx: {train_data_custom.class_to_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data_custom[0]\n",
    "print(f'Image shape: {img.shape}')\n",
    "print(f'Label: {label} ({train_data_custom.classes[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Created Successfully! ‚úÖ\n",
    "\n",
    "Perfect! Our custom dataset:\n",
    "- Returns tensors with correct shape `[3, 64, 64]`\n",
    "- Provides integer labels (0, 1, or 2)\n",
    "- Can be indexed like a Python list\n",
    "- Works exactly like PyTorch's built-in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare with ImageFolder\n",
    "\n",
    "Let's verify our custom dataset produces the same results as PyTorch's built-in `ImageFolder`.\n",
    "\n",
    "### Verification Checklist:\n",
    "\n",
    "| Property | Should Match |\n",
    "|----------|--------------|\n",
    "| Number of samples | `len(dataset)` |\n",
    "| Class names | `dataset.classes` |\n",
    "| Class indices | `dataset.class_to_idx` |\n",
    "| Sample format | `(tensor, label)` tuple |\n",
    "\n",
    "**Why this comparison matters:**\n",
    "- Ensures our implementation is correct\n",
    "- Validates that we understand how `ImageFolder` works internally\n",
    "- Confirms our dataset can be used as a drop-in replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_imagefolder = datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "print(f'Custom length: {len(train_data_custom)}')\n",
    "print(f'ImageFolder length: {len(train_data_imagefolder)}')\n",
    "print(f'Custom classes: {train_data_custom.classes}')\n",
    "print(f'ImageFolder classes: {train_data_imagefolder.classes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders\n",
    "\n",
    "Custom datasets work seamlessly with `DataLoader` - this is the power of PyTorch's modular design!\n",
    "\n",
    "### The Beauty of PyTorch's Design\n",
    "\n",
    "The `DataLoader` doesn't care whether you're using:\n",
    "- Built-in datasets (`ImageFolder`, `MNIST`, `CIFAR10`)\n",
    "- Custom datasets (like our `ImageFolderCustom`)\n",
    "- Any class that implements `__len__` and `__getitem__`\n",
    "\n",
    "**As long as your dataset implements the required methods correctly, everything just works!**\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "1. **Modularity** - Swap datasets without changing training code\n",
    "2. **Consistency** - Same DataLoader API for all datasets\n",
    "3. **Flexibility** - Build custom datasets for any use case\n",
    "4. **Integration** - Works with all PyTorch training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader_custom = DataLoader(\n",
    "    dataset=train_data_custom,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader_custom = DataLoader(\n",
    "    dataset=test_data_custom,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "images, labels = next(iter(train_dataloader_custom))\n",
    "print(f'Batch shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Custom Dataset\n",
    "\n",
    "Let's verify our custom dataset returns properly formatted images by visualizing a random sample.\n",
    "\n",
    "**What we're checking:**\n",
    "- Images display correctly (no corruption)\n",
    "- Transforms are applied properly (64√ó64 size)\n",
    "- Labels match the images\n",
    "- Random sampling works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_images(dataset, classes, n=6, seed=42):\n",
    "    random.seed(seed)\n",
    "    random_idx = random.sample(range(len(dataset)), k=n)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    for i, idx in enumerate(random_idx):\n",
    "        img, label = dataset[idx]\n",
    "        ax = axes[i // 3, i % 3]\n",
    "        ax.imshow(img.permute(1, 2, 0))\n",
    "        ax.set_title(f'{classes[label]}')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_images(train_data_custom, train_data_custom.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Working Perfectly! üéâ\n",
    "\n",
    "Excellent! Our `ImageFolderCustom` dataset is working exactly as expected:\n",
    "- Images are loaded correctly from disk\n",
    "- Transforms are applied (Resize, RandomFlip, ToTensor)\n",
    "- Labels match the visual content\n",
    "- Ready for training with DataLoader\n",
    "\n",
    "We've successfully built a custom dataset from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Three Essential Methods\n",
    "\n",
    "| Method | Purpose | Implementation |\n",
    "|--------|---------|----------------|\n",
    "| **`__init__`** | Setup: store paths, find classes, define transforms | Initialize all attributes |\n",
    "| **`__len__`** | Return `len(self.paths)` - total samples | Enable `len(dataset)` |\n",
    "| **`__getitem__`** | Load image, apply transform, return (tensor, label) | Enable indexing `dataset[i]` |\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Custom Dataset Structure** - How to subclass `torch.utils.data.Dataset`\n",
    "2. **Helper Functions** - Using `find_classes()` to discover labels\n",
    "3. **Transform Separation** - Different transforms for train/test\n",
    "4. **DataLoader Integration** - Custom datasets work seamlessly\n",
    "5. **Verification** - Comparing with built-in `ImageFolder`\n",
    "\n",
    "### When to Use Custom vs ImageFolder\n",
    "\n",
    "| Use Case | Recommendation | Reason |\n",
    "|----------|----------------|--------|\n",
    "| Standard image classification folders | `ImageFolder` | Fast, tested, optimized |\n",
    "| Annotations in CSV/JSON | Custom Dataset | Need custom label parsing |\n",
    "| Multiple input types (image + text) | Custom Dataset | Flexible data handling |\n",
    "| Complex preprocessing pipeline | Custom Dataset | Full control over loading |\n",
    "| Learning PyTorch internals | Custom Dataset | Educational value |\n",
    "\n",
    "### Custom Dataset Advantages\n",
    "\n",
    "‚úÖ **Complete control** over data loading process  \n",
    "‚úÖ **Flexible** - handle any data format or structure  \n",
    "‚úÖ **Customizable** - add special preprocessing logic  \n",
    "‚úÖ **Educational** - understand PyTorch internals  \n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Lab 4**, we'll build and train a **TinyVGG** convolutional neural network using our complete data pipeline!\n",
    "\n",
    "**The full workflow:**\n",
    "```\n",
    "Custom Dataset ‚Üí DataLoader ‚Üí TinyVGG Model ‚Üí Training ‚Üí Evaluation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Remember:** The power of custom datasets is flexibility. While `ImageFolder` is great for standard use cases, custom datasets unlock PyTorch's full potential for complex data scenarios!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
