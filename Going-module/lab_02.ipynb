{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Training Engine\n",
    "\n",
    "In this lab, we'll create `engine.py` - the heart of our training pipeline. This script contains the functions that actually train and evaluate our model.\n",
    "\n",
    "We'll build:\n",
    "- **`train_step()`**: Train the model for one epoch\n",
    "- **`test_step()`**: Evaluate the model for one epoch\n",
    "- **`train()`**: Combine both for full training loop\n",
    "\n",
    "By the end of this lab, you'll have a reusable training engine that can train any PyTorch model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Get Data and Use Previous Scripts\n",
    "\n",
    "Let's use the scripts we created in Lab 1. First, we need to set up our data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/poridhioss/Introduction-to-Deep-Learning-with-Pytorch-Resources/main/Going-module/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "going_modular_path = Path(\"going_modular\")\n",
    "going_modular_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data_setup.py (from Lab 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model_builder.py (from Lab 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, \n",
    "                      stride=1, \n",
    "                      padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*13*13, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup, model_builder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=32,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Training Loop\n",
    "\n",
    "Before we create `engine.py`, let's understand what happens in a typical training loop.\n",
    "\n",
    "### Training Step (for one epoch):\n",
    "1. Set model to training mode (`model.train()`)\n",
    "2. Loop through batches:\n",
    "   - Forward pass: Get predictions\n",
    "   - Calculate loss\n",
    "   - Zero gradients\n",
    "   - Backward pass: Calculate gradients\n",
    "   - Optimizer step: Update weights\n",
    "   - Track metrics (loss, accuracy)\n",
    "\n",
    "### Testing Step (for one epoch):\n",
    "1. Set model to evaluation mode (`model.eval()`)\n",
    "2. Turn off gradients (`torch.inference_mode()`)\n",
    "3. Loop through batches:\n",
    "   - Forward pass: Get predictions\n",
    "   - Calculate loss\n",
    "   - Track metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create `train_step()` Function\n",
    "\n",
    "Let's first create the training step function in the notebook, then we'll add it to our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    loss_fn: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `train_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Run one training step\n",
    "train_loss, train_acc = train_step(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Train loss: {train_loss:.4f}\")\n",
    "print(f\"Train accuracy: {train_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create `test_step()` Function\n",
    "\n",
    "Now let's create the testing step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "\n",
    "    model.eval() \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "            \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `test_step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one test step\n",
    "test_loss, test_acc = test_step(\n",
    "    model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create `train()` Function\n",
    "\n",
    "Now let's combine both steps into a full training function that runs for multiple epochs.\n",
    "\n",
    "Here, we take trains and tests a PyTorch model.\n",
    "```\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained and tested.\n",
    "        train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "        test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "        loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "        epochs: An integer indicating how many epochs to train for.\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics. In the form:\n",
    "        {train_loss: [...], train_acc: [...], test_loss: [...], test_acc: [...]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.Module, \n",
    "    train_dataloader: torch.utils.data.DataLoader, \n",
    "    test_dataloader: torch.utils.data.DataLoader, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    device: torch.device\n",
    ") -> Dict[str, List]:\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Full Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train for 5 epochs\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Results:\")\n",
    "print(f\"Final train loss: {results['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final train accuracy: {results['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final test loss: {results['test_loss'][-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {results['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create `engine.py`\n",
    "\n",
    "Now let's save all our training functions to `engine.py`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/engine.py\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    loss_fn: torch.nn.Module, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    model: torch.nn.Module, \n",
    "    dataloader: torch.utils.data.DataLoader, \n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "\n",
    "    model.eval() \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "            \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module, \n",
    "    train_dataloader: torch.utils.data.DataLoader, \n",
    "    test_dataloader: torch.utils.data.DataLoader, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    epochs: int,\n",
    "    device: torch.device\n",
    ") -> Dict[str, List]:\n",
    "\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(\n",
    "            model=model,\n",
    "            dataloader=train_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device\n",
    "        )\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `engine.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import engine\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = engine.train(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Our Directory Structure\n",
    "\n",
    "Let's verify our `going_modular` directory now has all three scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Files in going_modular/:\")\n",
    "for file in os.listdir(\"going_modular\"):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we created `engine.py` - the training engine for our modular PyTorch project:\n",
    "\n",
    "1. **`train_step()`** - Trains the model for one epoch\n",
    "   - Sets model to train mode\n",
    "   - Performs forward pass, loss calculation, backward pass, optimizer step\n",
    "   - Returns average loss and accuracy\n",
    "\n",
    "2. **`test_step()`** - Evaluates the model for one epoch\n",
    "   - Sets model to eval mode\n",
    "   - Uses `torch.inference_mode()` for efficiency\n",
    "   - Returns average loss and accuracy\n",
    "\n",
    "3. **`train()`** - Full training loop\n",
    "   - Combines train_step and test_step\n",
    "   - Runs for specified number of epochs\n",
    "   - Returns results dictionary with metrics history\n",
    "\n",
    "Our directory structure now looks like:\n",
    "```\n",
    "going_modular/\n",
    "    ├── data_setup.py      # DataLoader creation\n",
    "    ├── model_builder.py   # TinyVGG model\n",
    "    └── engine.py          # Training functions\n",
    "```\n",
    "\n",
    "In the next lab, we'll create `utils.py` and `train.py` to complete our modular pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Add learning rate scheduling** - Modify the `train()` function to accept an optional learning rate scheduler and step it after each epoch.\n",
    "\n",
    "2. **Add early stopping** - Implement early stopping in the `train()` function that stops training if test loss doesn't improve for N epochs.\n",
    "\n",
    "3. **Add gradient clipping** - Modify `train_step()` to include gradient clipping to prevent exploding gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
