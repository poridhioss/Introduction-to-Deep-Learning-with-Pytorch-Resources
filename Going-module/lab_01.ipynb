{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Setup & Model Builder Scripts\n",
    "\n",
    "In this lab, you'll learn how to **turn notebook code into reusable Python scripts** - a key skill for moving from experimentation to production.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the benefits of modular code organization\n",
    "- Create `data_setup.py` for DataLoader creation\n",
    "- Create `model_builder.py` with the TinyVGG architecture\n",
    "- Import and use modular scripts in notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Go Modular?\n",
    "| Notebooks | Python Scripts |\n",
    "|-----------|----------------|\n",
    "| **Quick experimentation** - Test ideas rapidly | **Reusable code** - Write once, import anywhere (no copy-pasting!) |\n",
    "| **Visualization** - See plots inline | **Version control** - Track changes with git |\n",
    "| **Sharing ideas** - Show results easily | **Cloud & servers** - Run training jobs remotely |\n",
    "| **Learning & prototyping** - Ideal for iteration | **Production deployments** - Integrate into apps & pipelines |\n",
    "\n",
    "**Key insight:** Notebooks are great for exploration, but as your code matures, scripts make it easier to maintain, test, and scale your ML workflows.\n",
    "\n",
    "**The common pattern:** Start in notebooks, move to scripts when you have working code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download the Dataset\n",
    "\n",
    "We'll use the **pizza_steak_sushi** dataset - a small image classification dataset with 3 classes.\n",
    "\n",
    "**Dataset Structure (ImageFolder format):**\n",
    "```\n",
    "data/\n",
    "└── pizza_steak_sushi/\n",
    "    ├── train/\n",
    "    │   ├── pizza/\n",
    "    │   ├── steak/\n",
    "    │   └── sushi/\n",
    "    └── test/\n",
    "        ├── pizza/\n",
    "        ├── steak/\n",
    "        └── sushi/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/poridhioss/Introduction-to-Deep-Learning-with-Pytorch-Resources/main/Going-module/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
    "    print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DataLoaders (Traditional Way)\n",
    "\n",
    "Before creating our modular script, let's see the standard notebook approach:\n",
    "\n",
    "1. Define transforms\n",
    "2. Create datasets using `ImageFolder`\n",
    "3. Wrap datasets in `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_data)}\")\n",
    "print(f\"Test dataset size: {len(test_data)}\")\n",
    "print(f\"Classes: {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a batch\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create `data_setup.py`\n",
    "\n",
    "Now let's **convert this code into a reusable function** and save it as `data_setup.py`.\n",
    "\n",
    "**Function:** `create_dataloaders()`\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `train_dir` | Path to training directory |\n",
    "| `test_dir` | Path to testing directory |\n",
    "| `transform` | Torchvision transforms to apply |\n",
    "| `batch_size` | Samples per batch |\n",
    "| `num_workers` | Workers for data loading |\n",
    "\n",
    "**Returns:** `(train_dataloader, test_dataloader, class_names)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create going_modular directory\n",
    "going_modular_path = Path(\"going_modular\")\n",
    "going_modular_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created directory: {going_modular_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `data_setup.py`\n",
    "\n",
    "Let's verify our module works by importing and using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build TinyVGG Model (Traditional Way)\n",
    "\n",
    "**TinyVGG** is a simplified VGG architecture:\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Conv Block 1 | 2x Conv2d + ReLU + MaxPool |\n",
    "| Conv Block 2 | 2x Conv2d + ReLU + MaxPool |\n",
    "| Classifier | Flatten + Linear |\n",
    "\n",
    "This architecture is small enough to train quickly but powerful enough for real classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, \n",
    "                      stride=1, \n",
    "                      padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Note: The in_features depends on the input image size\n",
    "            # For 64x64 images: 13*13*hidden_units\n",
    "            nn.Linear(in_features=hidden_units*13*13, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = TinyVGG(\n",
    "    input_shape=3,  # RGB images\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)  # 3 classes\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample batch\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = model(images)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create `model_builder.py`\n",
    "\n",
    "Save our TinyVGG model to a reusable Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3, \n",
    "                      stride=1, \n",
    "                      padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
    "            nn.Linear(in_features=hidden_units*13*13, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        # Alternatively: return self.classifier(self.conv_block_2(self.conv_block_1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `model_builder.py`\n",
    "\n",
    "Import and verify the model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular import model_builder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(images)\n",
    "    \n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (first 3 samples):\\n{output[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verify Directory Structure\n",
    "\n",
    "Confirm both scripts were created successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in going_modular/:\")\n",
    "for file in os.listdir(\"going_modular\"):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how to go modular with PyTorch:\n",
    "\n",
    "| Step | What You Created | Purpose |\n",
    "|------|------------------|---------|\n",
    "| 1 | Downloaded data | pizza_steak_sushi dataset |\n",
    "| 2 | `data_setup.py` | `create_dataloaders()` function |\n",
    "| 3 | `model_builder.py` | `TinyVGG` class |\n",
    "\n",
    "**Final Directory Structure:**\n",
    "```\n",
    "going_modular/\n",
    "├── data_setup.py      # DataLoader creation\n",
    "└── model_builder.py   # TinyVGG model\n",
    "```\n",
    "\n",
    "**Next:** In Lab 2, you'll create `engine.py` with training and testing functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify `data_setup.py`** to accept separate transforms for training and testing data (training might use data augmentation while testing should not).\n",
    "\n",
    "2. **Add a new model** to `model_builder.py` called `TinyVGG_v2` that uses `BatchNorm2d` after each convolutional layer.\n",
    "\n",
    "3. **Create a `get_data.py` script** that downloads the data if it doesn't exist. This could be imported in `train.py` later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
