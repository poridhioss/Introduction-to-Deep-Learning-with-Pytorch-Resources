{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Building and Deploying Gradio Demos\n",
    "\n",
    "In this lab, we will build interactive Gradio demos for our food classification models and deploy them to Hugging Face Spaces.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Build interactive web demos using Gradio\n",
    "- Structure a deployable ML application\n",
    "- Deploy models to Hugging Face Spaces\n",
    "- Scale up to larger datasets (Food101) with Food Classifier Big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "First, let's set up our environment and install Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: torchinfo in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: gradio in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (6.3.0)\n",
      "Requirement already satisfied: filelock in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (4.12.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.128.0)\n",
      "Requirement already satisfied: ffmpy in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.3 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: groovy~=0.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (1.3.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (2.12.5)\n",
      "Requirement already satisfied: pydub in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.0.21)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.21.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: idna>=2.8 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: shellingham in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: colorama in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\poridhi-ai-labs\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchinfo gradio\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\hamburger_hot_dog_french_fries directory exists.\n",
      "Train directory: data\\hamburger_hot_dog_french_fries\\train\n",
      "Test directory: data\\hamburger_hot_dog_french_fries\\test\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "\n",
    "data_path = Path('data/')\n",
    "image_path = data_path / 'hamburger_hot_dog_french_fries'\n",
    "\n",
    "if image_path.is_dir() and (image_path / 'train').is_dir():\n",
    "    print(f'{image_path} directory exists.')\n",
    "else:\n",
    "    print(f'Setting up {image_path} directory...')\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    zip_file_path = data_path / 'hamburger_hot_dog_french_fries.zip'\n",
    "    with open(zip_file_path, 'wb') as f:\n",
    "        request = requests.get('https://github.com/poridhioss/Introduction-to-Deep-Learning-with-Pytorch-Resources/raw/refs/heads/main/Model-deployment/hamburger_hot_dog_french_fries.zip')\n",
    "        print('Downloading data...')\n",
    "        f.write(request.content)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        print('Unzipping data...')\n",
    "        # Extract to data_path since zip already contains the hamburger_hot_dog_french_fries folder\n",
    "        zip_ref.extractall(data_path)\n",
    "    print('Done!')\n",
    "\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "print(f'Train directory: {train_dir}')\n",
    "print(f'Test directory: {test_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EffNetB2 model created!\n",
      "Class names: ['french_fries', 'hamburger', 'hot_dog']\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def create_effnetb2_model(num_classes: int = 3, seed: int = 42):\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "    \n",
    "    # Freeze base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes)\n",
    "    )\n",
    "    \n",
    "    return model, transforms\n",
    "\n",
    "# Note: ImageFolder loads classes in alphabetical order\n",
    "class_names = [\"french_fries\", \"hamburger\", \"hot_dog\"]\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3)\n",
    "print('EffNetB2 model created!')\n",
    "print(f'Class names: {class_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int = os.cpu_count()\n",
    ") -> Tuple[DataLoader, DataLoader, List[str]]:\n",
    "    from torchvision import datasets\n",
    "    \n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "    return train_loss / len(dataloader), train_acc / len(dataloader)\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
    "    return test_loss / len(dataloader), test_acc / len(dataloader)\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device):\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "        print(f\"Epoch: {epoch+1} | train_loss: {train_loss:.4f} | train_acc: {train_acc:.4f} | test_loss: {test_loss:.4f} | test_acc: {test_acc:.4f}\")\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    return results\n",
    "\n",
    "def save_model(model, target_dir, model_name):\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_save_path = target_dir_path / model_name\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(), f=model_save_path)\n",
    "\n",
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "    import matplotlib.pyplot as plt\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"[INFO] Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating DataLoaders...\n",
      "[INFO] Class names: ['french_fries', 'hamburger', 'hot_dog']\n",
      "[INFO] Training samples: 2250\n",
      "[INFO] Testing samples: 750\n",
      "\n",
      "[INFO] Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]f:\\poridhi-ai-labs\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      " 20%|‚ñà‚ñà        | 1/5 [05:33<22:15, 333.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6655 | train_acc: 0.7670 | test_loss: 0.3916 | test_acc: 0.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [10:48<16:06, 322.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | train_loss: 0.4185 | train_acc: 0.8691 | test_loss: 0.3178 | test_acc: 0.9051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [16:05<10:40, 320.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | train_loss: 0.3620 | train_acc: 0.8739 | test_loss: 0.2925 | test_acc: 0.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [21:39<05:25, 325.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | train_loss: 0.3168 | train_acc: 0.8934 | test_loss: 0.2715 | test_acc: 0.9129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [26:56<00:00, 323.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | train_loss: 0.3157 | train_acc: 0.8864 | test_loss: 0.2704 | test_acc: 0.9185\n",
      "[INFO] Saving model to: models\\pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\n",
      "\n",
      "[INFO] Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Creating DataLoaders...\")\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=effnetb2_transforms,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Class names: {class_names}\")\n",
    "print(f\"[INFO] Training samples: {len(train_dataloader.dataset)}\")\n",
    "print(f\"[INFO] Testing samples: {len(test_dataloader.dataset)}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(params=effnetb2.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n[INFO] Training model...\")\n",
    "effnetb2_results = train(\n",
    "    model=effnetb2,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "save_model(\n",
    "    model=effnetb2,\n",
    "    target_dir=\"models\",\n",
    "    model_name=\"pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\"\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bringing Food Classifier to life by creating a Gradio demo\n",
    "\n",
    "We've decided we'd like to deploy the EffNetB2 model (to begin with, this could always be changed later).\n",
    "\n",
    "So how can we do that?\n",
    "\n",
    "There are several ways to deploy a machine learning model each with specific use cases (as discussed above).\n",
    "\n",
    "We're going to be focused on perhaps the quickest and certainly one of the most fun ways to get a model deployed to the internet.\n",
    "\n",
    "And that's by using [Gradio](https://gradio.app/).\n",
    "\n",
    "Why create a demo of your models?\n",
    "\n",
    "Because metrics on the test set look nice but you never really know how your model performs until you use it in the wild.\n",
    "\n",
    "![Gradio Workflow](https://raw.githubusercontent.com/poridhiEng/lab-asset/cdf9e5ca3eda11102b5f8493572eb84e83523d47/tensorcode/Deep-learning-with-pytorch/Model-Deployment/Lab_02/images/img4.svg)\n",
    "\n",
    "So let's get deploying!\n",
    "\n",
    "We'll start by importing Gradio with the common alias `gr` and if it's not present, we'll install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio version: 6.3.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradio version: {gr.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio ready!\n",
    "\n",
    "Let's turn Food Classifier into a demo application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Creating a function to map our inputs and outputs\n",
    "\n",
    "To create our Food Classifier demo with Gradio, we'll need a function to map our inputs to our outputs.\n",
    "\n",
    "We created a function earlier called `pred_and_store()` to make predictions with a given model across a list of target files and store them in a list of dictionaries.\n",
    "\n",
    "How about we create a similar function but this time focusing on making a prediction on a single image with our EffNetB2 model?\n",
    "\n",
    "More specifically, we want a function that takes an image as input, preprocesses (transforms) it, makes a prediction with EffNetB2 and then returns the prediction (pred or pred label for short) as well as the prediction probability (pred prob).\n",
    "\n",
    "And while we're here, let's return the time it took to do so too:\n",
    "\n",
    "```\n",
    "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
    "```\n",
    "\n",
    "This will be our `fn` parameter for our Gradio interface.\n",
    "\n",
    "First, let's make sure our EffNetB2 model is on the CPU (since we're sticking with CPU-only predictions, however you could change this if you have access to a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2.to(\"cpu\") \n",
    "\n",
    "next(iter(effnetb2.parameters())).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create a function called `predict()` to replicate the workflow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "\n",
    "    start_time = timer()\n",
    "    \n",
    "    # Transform the target image and add a batch dimension\n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    \n",
    "    # Put model into evaluation mode and turn on inference mode\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! \n",
    "\n",
    "Now let's see our function in action by performing a prediction on a random image from the test dataset.\n",
    "\n",
    "We'll start by getting a list of all the image paths from the test directory and then randomly selecting one.\n",
    "\n",
    "Then we'll open the randomly selected image with `PIL.Image.open()`.\n",
    "\n",
    "Finally, we'll pass the image to our `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Predicting on image at path: data\\hamburger_hot_dog_french_fries\\test\\hot_dog\\156135.jpg\n",
      "\n",
      "Prediction label and probability dictionary: \n",
      "{'french_fries': 0.0037253673654049635, 'hamburger': 0.03534900024533272, 'hot_dog': 0.9609256386756897}\n",
      "Prediction time: 0.17936 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "\n",
    "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
    "\n",
    "image = Image.open(random_image_path)\n",
    "print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n",
    "\n",
    "pred_dict, pred_time = predict(img=image)\n",
    "print(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\n",
    "print(f\"Prediction time: {pred_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Running the cell above a few times we can see different prediction probabilities for each label from our EffNetB2 model as well as the time it took per prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Creating a list of example images\n",
    "\n",
    "Our `predict()` function enables us to go from inputs -> transform -> ML model -> outputs.\n",
    "\n",
    "Which is exactly what we need for our Graido demo.\n",
    "\n",
    "But before we create the demo, let's create one more thing: a list of examples.\n",
    "\n",
    "Gradio's `Interface` class takes a list of `examples` of as an optional parameter (`gradio.Interface(examples=List[Any])`).\n",
    "\n",
    "And the format for the `examples` parameter is a list of lists.\n",
    "\n",
    "So let's create a list of lists containing random filepaths to our test images.\n",
    "\n",
    "Three examples should be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['data\\\\hamburger_hot_dog_french_fries\\\\test\\\\hamburger\\\\1960715.jpg'],\n",
       " ['data\\\\hamburger_hot_dog_french_fries\\\\test\\\\hamburger\\\\2007244.jpg'],\n",
       " ['data\\\\hamburger_hot_dog_french_fries\\\\test\\\\french_fries\\\\2823700.jpg']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!\n",
    "\n",
    "Our Gradio demo will showcase these as example inputs to our demo so people can try it out and see what it does without uploading any of their own data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Building a Gradio interface\n",
    "\n",
    "Time to put everything together and bring our Food Classifier demo to life!\n",
    "\n",
    "Let's create a Gradio interface to replicate the workflow:\n",
    "\n",
    "```\n",
    "input: image -> transform -> predict with EffNetB2 -> output: pred, pred prob, time taken\n",
    "```\n",
    "\n",
    "We can do with the `gradio.Interface()` class with the following parameters:\n",
    "* `fn` - a Python function to map `inputs` to `outputs`, in our case, we'll use our `predict()` function.\n",
    "* `inputs` - the input to our interface, such as an image using `gradio.Image()` or `\"image\"`. \n",
    "* `outputs` - the output of our interface once the `inputs` have gone through the `fn`, such as a label using `gradio.Label()` (for our model's predicted labels) or number using `gradio.Number()` (for our model's prediction time).\n",
    "    * **Note:** Gradio comes with many in-built `inputs` and `outputs` options known as \"Components\".\n",
    "* `examples` - a list of examples to showcase for the demo.\n",
    "* `title` - a string title of the demo.\n",
    "* `description` - a string description of the demo.\n",
    "* `article` - a reference note at the bottom of the demo.\n",
    "\n",
    "Once we've created our demo instance of `gr.Interface()`, we can bring it to life using `gradio.Interface().launch()` or `demo.launch()` command. \n",
    "\n",
    "Easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://ed9bf612f4564bd301.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ed9bf612f4564bd301.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "title = \"Food Classifier üçîüå≠üçü\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as hamburger, hot dog or french fries.\"\n",
    "article = \"Created at [PyTorch Model Deployment](https://www.learnpytorch.io/pytorch_model_deployment/).\"\n",
    "\n",
    "demo = gr.Interface(fn=predict, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!!! What an epic demo!!!\n",
    "\n",
    "Food Classifier has officially come to life in an interface someone could use and try out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Turning our Food Classifier Gradio Demo into a deployable app\n",
    "\n",
    "We've seen our Food Classifier model come to life through a Gradio demo.\n",
    "\n",
    "But what if we wanted to share it with our friends?\n",
    "\n",
    "Well, we could use the provided Gradio link, however, the shared link only lasts for 72-hours.\n",
    "\n",
    "To make our Food Classifier demo more permanent, we can package it into an app and upload it to [Hugging Face Spaces](https://huggingface.co/spaces/launch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Deployed Gradio app structure\n",
    "\n",
    "To upload our demo Gradio app, we'll want to put everything relating to it into a single directory.\n",
    "\n",
    "For example, our demo might live at the path `demos/food_classifier/` with the file structure:\n",
    "\n",
    "```\n",
    "demos/\n",
    "‚îî‚îÄ‚îÄ food_classifier/\n",
    "    ‚îú‚îÄ‚îÄ pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\n",
    "    ‚îú‚îÄ‚îÄ app.py\n",
    "    ‚îú‚îÄ‚îÄ examples/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ example_1.jpg\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ example_2.jpg\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ example_3.jpg\n",
    "    ‚îú‚îÄ‚îÄ model.py\n",
    "    ‚îî‚îÄ‚îÄ requirements.txt\n",
    "```\n",
    "\n",
    "Where:\n",
    "* `pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth` is our trained PyTorch model file.\n",
    "* `app.py` contains our Gradio app (similar to the code that launched the app).\n",
    "    * **Note:** `app.py` is the default filename used for Hugging Face Spaces, if you deploy your app there, Spaces will by default look for a file called `app.py` to run. This is changeable in settings.\n",
    "* `examples/` contains example images to use with our Gradio app.\n",
    "* `model.py` contains the model definition as well as any transforms associated with the model.\n",
    "* `requirements.txt` contains the dependencies to run our app such as `torch`, `torchvision` and `gradio`.\n",
    "\n",
    "Why this way?\n",
    "\n",
    "Because it's one of the simplest layouts we could begin with. \n",
    "\n",
    "Our focus is: *experiment, experiment, experiment!* \n",
    "\n",
    "The quicker we can run smaller experiments, the better our bigger ones will be.\n",
    "\n",
    "Here's a visual representation of the deployment structure:\n",
    "\n",
    "![Deployment Structure](https://raw.githubusercontent.com/poridhiEng/lab-asset/cdf9e5ca3eda11102b5f8493572eb84e83523d47/tensorcode/Deep-learning-with-pytorch/Model-Deployment/Lab_02/images/img2.svg)\n",
    "\n",
    "<!-- We're going to work towards recreating the structure above but you can see a live demo app running on Hugging Face Spaces as well as the file structure:\n",
    "* [Live Gradio demo of Food Classifier üçîüå≠üçü](https://huggingface.co/spaces/mrdbourke/food_classifier).\n",
    "* [Food Classifier file structure on Hugging Face Spaces](https://huggingface.co/spaces/mrdbourke/food_classifier/tree/main). -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Creating a `demos` folder to store our Food Classifier app files\n",
    "\n",
    "To begin, let's first create a `demos/` directory to store all of our Food Classifier app files.\n",
    "\n",
    "We can do with Python's `pathlib.Path(\"path_to_dir\")` to establish the directory path and `pathlib.Path(\"path_to_dir\").mkdir()` to create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "food_classifier_demo_path = Path(\"demos/food_classifier/\")\n",
    "\n",
    "if food_classifier_demo_path.exists():\n",
    "    shutil.rmtree(food_classifier_demo_path)\n",
    "food_classifier_demo_path.mkdir(parents=True, \n",
    "                                exist_ok=True)\n",
    "    \n",
    "import os\n",
    "print(os.listdir(\"demos/food_classifier/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Creating a folder of example images to use with our Food Classifier demo\n",
    "\n",
    "Now we've got a directory to store our Food Classifier demo files, let's add some examples to it.\n",
    "\n",
    "Three example images from the test dataset should be enough.\n",
    "\n",
    "To do so we'll:\n",
    "1. Create an `examples/` directory within the `demos/food_classifier` directory.\n",
    "2. Choose three random images from the test dataset and collect their filepaths in a list.\n",
    "3. Copy the three random images from the test dataset to the `demos/food_classifier/examples/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Copying data\\hamburger_hot_dog_french_fries\\test\\french_fries\\1008163.jpg to demos\\food_classifier\\examples\\1008163.jpg\n",
      "[INFO] Copying data\\hamburger_hot_dog_french_fries\\test\\french_fries\\1033213.jpg to demos\\food_classifier\\examples\\1033213.jpg\n",
      "[INFO] Copying data\\hamburger_hot_dog_french_fries\\test\\french_fries\\10500.jpg to demos\\food_classifier\\examples\\10500.jpg\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "food_classifier_examples_path = food_classifier_demo_path / \"examples\"\n",
    "food_classifier_examples_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "test_images = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
    "if len(test_images) >= 3:\n",
    "    food_classifier_examples = test_images[:3]\n",
    "else:\n",
    "    print(f\"[WARNING] Only found {len(test_images)} test images\")\n",
    "    food_classifier_examples = test_images\n",
    "\n",
    "for example in food_classifier_examples:\n",
    "    destination = food_classifier_examples_path / example.name\n",
    "    print(f\"[INFO] Copying {example} to {destination}\")\n",
    "    shutil.copy2(src=example, dst=destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to verify our examples are present, let's list the contents of our `demos/food_classifier/examples/` directory with `os.listdir()` and then format the filepaths into a list of lists (so it's compatible with Gradio's `gradio.Interface()` `example` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['examples/1008163.jpg'], ['examples/1033213.jpg'], ['examples/10500.jpg']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(food_classifier_examples_path)]\n",
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Moving our trained EffNetB2 model to our Food Classifier demo directory\n",
    "\n",
    "We previously saved our Food Classifier EffNetB2 feature extractor model under `models/pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth`.\n",
    "\n",
    "And rather double up on saved model files, let's move our model to our `demos/food_classifier` directory.\n",
    "\n",
    "We can do so using Python's `shutil.copy2()` method and passing in `src` (the source path of the target file) and `dst` (the destination path of the target file to be copied to) parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Attempting to copy models\\pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth to demos\\food_classifier\\pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\n",
      "[INFO] Model copy complete.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "effnetb2_food_classifier_model_path = Path(\"models/pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\")\n",
    "\n",
    "effnetb2_food_classifier_model_destination = food_classifier_demo_path / effnetb2_food_classifier_model_path.name\n",
    "\n",
    "try:\n",
    "    print(f\"[INFO] Attempting to copy {effnetb2_food_classifier_model_path} to {effnetb2_food_classifier_model_destination}\")\n",
    "    \n",
    "    shutil.copy2(src=effnetb2_food_classifier_model_path, \n",
    "                 dst=effnetb2_food_classifier_model_destination)\n",
    "    \n",
    "    print(f\"[INFO] Model copy complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"[INFO] No model found at {effnetb2_food_classifier_model_path}\")\n",
    "    print(f\"[INFO] Please make sure you've run the training cells above first.\")\n",
    "except Exception as e:\n",
    "    print(f\"[INFO] Error: {e}\")\n",
    "    print(f\"[INFO] Model exists at {effnetb2_food_classifier_model_destination}: {effnetb2_food_classifier_model_destination.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Turning our EffNetB2 model into a Python script (`model.py`)\n",
    "\n",
    "Our current model's `state_dict` is saved to `demos/food_classifier/pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth`.\n",
    "\n",
    "To load it in we can use `model.load_state_dict()` along with `torch.load()`.\n",
    "\n",
    "\n",
    "But before we can do this, we first need a way to instantiate a `model`.\n",
    "\n",
    "To do this in a modular fashion we'll create a script called `model.py` which contains our `create_effnetb2_model()` function.\n",
    "\n",
    "That way we can import the function in *another* script (see `app.py` below) and then use it to create our EffNetB2 `model` instance as well as get its appropriate transforms.\n",
    "\n",
    "Here, we'll use the `%%writefile path/to/file` magic command to turn a cell of code into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/food_classifier/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/food_classifier/model.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def create_effnetb2_model(num_classes:int=3, seed:int=42):\n",
    "\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms = weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights)\n",
    "\n",
    "    # Freeze all layers in base model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=num_classes),\n",
    "    )\n",
    "    \n",
    "    return model, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Turning our Food Classifier Gradio app into a Python script (`app.py`)\n",
    "\n",
    "We've now got a `model.py` script as well as a path to a saved model `state_dict` that we can load in.\n",
    "\n",
    "Time to construct `app.py`.\n",
    "\n",
    "We call it `app.py` because by default when you create a HuggingFace Space, it looks for a file called `app.py` to run and host (though you can change this in settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/food_classifier/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/food_classifier/app.py\n",
    "import gradio as gr\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from model import create_effnetb2_model\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class_names = [\"french_fries\", \"hamburger\", \"hot_dog\"]\n",
    "\n",
    "effnetb2, effnetb2_transforms = create_effnetb2_model(\n",
    "    num_classes=3, \n",
    ")\n",
    "\n",
    "effnetb2.load_state_dict(\n",
    "    torch.load(\n",
    "        f=\"pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\",\n",
    "        map_location=torch.device(\"cpu\"), \n",
    "    )\n",
    ")\n",
    "\n",
    "def predict(img) -> Tuple[Dict, float]:\n",
    "\n",
    "    start_time = timer()\n",
    "    \n",
    "    img = effnetb2_transforms(img).unsqueeze(0)\n",
    "    effnetb2.eval()\n",
    "    with torch.inference_mode():\n",
    "        pred_probs = torch.softmax(effnetb2(img), dim=1)\n",
    "    \n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    return pred_labels_and_probs, pred_time\n",
    "\n",
    "title = \"Food Classifier üçîüå≠üçü\"\n",
    "description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as hamburger, hot dog or french fries.\"\n",
    "article = \"Created at PyTorch Model Deployment.\"\n",
    "\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "demo = gr.Interface(fn=predict, \n",
    "                    inputs=gr.Image(type=\"pil\"),\n",
    "                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), \n",
    "                             gr.Number(label=\"Prediction time (s)\")], \n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Creating a requirements file for Food Classifier (`requirements.txt`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing demos/food_classifier/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile demos/food_classifier/requirements.txt\n",
    "torch>=2.0.0\n",
    "torchvision>=0.15.0\n",
    "gradio>=4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "We've officially got all the files we need to deploy our Food Classifier demo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Deploying our Food Classifier app to HuggingFace Spaces\n",
    "\n",
    "We've got a file containing our Food Classifier demo, now how do we get it to run on Hugging Face Spaces?\n",
    "\n",
    "There are two main options for uploading to a Hugging Face Space (also called a [Hugging Face Repository](https://huggingface.co/docs/hub/repositories-getting-started#getting-started-with-repositories), similar to a git repository): \n",
    "1. [Uploading via the Hugging Face Web interface (easiest)](https://huggingface.co/docs/hub/repositories-getting-started#adding-files-to-a-repository-web-ui).\n",
    "2. [Uploading via the command line or terminal](https://huggingface.co/docs/hub/repositories-getting-started#terminal).\n",
    "    * **Bonus:** You can also use the [`huggingface_hub` library](https://huggingface.co/docs/huggingface_hub/index) to interact with Hugging Face, this would be a good extension to the above two options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Downloading our Food Classifier app files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To begin uploading our files to Hugging Face, let's now download them from Google Colab (or wherever you're running this notebook).\n",
    "\n",
    "To do so, we'll first compress the files into a single zip folder via the command: \n",
    "\n",
    "```\n",
    "zip -r ../food_classifier.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n",
    "```\n",
    "\n",
    "Where: \n",
    "* `zip` stands for \"zip\" as in \"please zip together the files in the following directory\". \n",
    "* `-r` stands for \"recursive\" as in, \"go through all of the files in the target directory\".\n",
    "* `../food_classifier.zip` is the target directory we'd like our files to be zipped to.\n",
    "* `*` stands for \"all the files in the current directory\".\n",
    "* `-x` stands for \"exclude these files\". \n",
    "\n",
    "We can download our zip file from Google Colab using [`google.colab.files.download(\"demos/food_classifier.zip\")`](https://colab.research.google.com/notebooks/io.ipynb) (we'll put this inside a `try` and `except` block just in case we're not running the code inside Google Colab, and if so we'll print a message saying to manually download the files).\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding: app.py\n",
      "Adding: model.py\n",
      "Adding: pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\n",
      "Adding: requirements.txt\n",
      "Adding: examples\\1008163.jpg\n",
      "Adding: examples\\1033213.jpg\n",
      "Adding: examples\\10500.jpg\n",
      "\n",
      "Created: demos\\food_classifier.zip\n",
      "Not running in Google Colab, can't use google.colab.files.download(), please manually download.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create zip file using Python (cross-platform)\n",
    "zip_path = Path(\"demos/food_classifier.zip\")\n",
    "source_dir = Path(\"demos/food_classifier\")\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        # Skip unwanted directories\n",
    "        dirs[:] = [d for d in dirs if d not in ['__pycache__', '.ipynb_checkpoints']]\n",
    "        for file in files:\n",
    "            if not file.endswith(('.pyc', '.ipynb')):\n",
    "                file_path = Path(root) / file\n",
    "                arcname = file_path.relative_to(source_dir)\n",
    "                print(f\"Adding: {arcname}\")\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"\\nCreated: {zip_path}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"demos/food_classifier.zip\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!\n",
    "\n",
    "Looks like our `zip` command was successful.\n",
    "\n",
    "If you're running this notebook in Google Colab, you should see a file start to download in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.2 Running our Food Classifier demo locally\n",
    "\n",
    "If you download the `food_classifier.zip` file, you can test it locally by:\n",
    "1. Unzipping the file.\n",
    "2. Opening terminal or a command line prompt.\n",
    "3. Changing into the `food_classifier` directory (`cd food_classifier`).\n",
    "4. Creating an environment (`python3 -m venv env`).\n",
    "5. Activating the environment (`source env/bin/activate`).\n",
    "5. Installing the requirements (`pip install -r requirements.txt`, the \"`-r`\" is for recursive).\n",
    "    * **Note:** This step may take 5-10 minutes depending on your internet connection. And if you're facing errors, you may need to upgrade `pip` first: `pip install --upgrade pip`.\n",
    "6. Run the app (`python3 app.py`).\n",
    "\n",
    "This should result in a Gradio demo just like the one we built above running locally on your machine at a URL such as `http://127.0.0.1:7860/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9.3 Uploading to Hugging Face\n",
    "\n",
    "We've verified our Food Classifier app works locally, however, the fun of creating a machine learning demo is to show it to other people and allow them to use it.\n",
    "\n",
    "To do so, we're going to upload our Food Classifier demo to Hugging Face. \n",
    "\n",
    "1. [Sign up](https://huggingface.co/join) for a Hugging Face account. \n",
    "2. Start a new Hugging Face Space by going to your profile and then [clicking \"New Space\"](https://huggingface.co/new-space).\n",
    "    * **Note:** A Space in Hugging Face is also known as a \"code repository\" (a place to store your code/files) or \"repo\" for short.\n",
    "3. Give the Space a name, for example, mine is called `poridhi001/food_classifier`, you can see it here: https://huggingface.co/spaces/poridhi001/food_classifier\n",
    "4. Select a license (I used [MIT](https://opensource.org/licenses/MIT)).\n",
    "5. Select Gradio as the Space SDK (software development kit). \n",
    "   * **Note:** You can use other options such as Streamlit but since our app is built with Gradio, we'll stick with that.\n",
    "6. Choose whether your Space is it's public or private (I selected public since I'd like my Space to be available to others).\n",
    "7. Click \"Create Space\".\n",
    "8. Clone the repo locally by running something like: `git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]` in terminal or command prompt.\n",
    "    * **Note:** You can also add files via uploading them under the \"Files and versions\" tab.\n",
    "9. Copy/move the contents of the downloaded `food_classifier` folder to the cloned repo folder.\n",
    "10. To upload and track larger files (e.g. files over 10MB or in our case, our PyTorch model file) you'll need to [install Git LFS](https://git-lfs.github.com/) (which stands for \"git large file storage\").\n",
    "11. After you've installed Git LFS, you can activate it by running `git lfs install`.\n",
    "12. In the `food_classifier` directory, track the files over 10MB with Git LFS with `git lfs track \"*.file_extension\"`.\n",
    "    * Track EffNetB2 PyTorch model file with `git lfs track \"pretrained_effnetb2_feature_extractor_hamburger_hot_dog_french_fries.pth\"`.\n",
    "13. Track `.gitattributes` (automatically created when cloning from HuggingFace, this file will help ensure our larger files are tracked with Git LFS). You can see an example `.gitattributes` file on the [Food Classifier Hugging Face Space](https://huggingface.co/spaces/poridhi001/food_classifier/blob/main/.gitattributes).\n",
    "    * `git add .gitattributes`\n",
    "14. Add the rest of the `food_classifier` app files and commit them with: \n",
    "    * `git add *`\n",
    "    * `git commit -m \"first commit\"`\n",
    "15. Push (upload) the files to Hugging Face:\n",
    "    * `git push`\n",
    "16. Wait 3-5 minutes for the build to happen (future builds are faster) and your app to become live!\n",
    "\n",
    "If everything worked, you should see a live running example of our Food Classifier Gradio demo like the one here: https://huggingface.co/spaces/poridhi001/food_classifier\n",
    "\n",
    "![Deployed Food Classifier Demo on HuggingFace Spaces](https://github.com/poridhiEng/lab-asset/blob/main/tensorcode/Deep-learning-with-pytorch/Model-Deployment/Lab_02/images/img5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Gradio makes it easy to create interactive ML demos\n",
    "- Hugging Face Spaces provides free hosting for ML applications\n",
    "- Proper app structure includes: model.py, app.py, requirements.txt\n",
    "- Deployed models can be shared and used by anyone with the URL\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the PyTorch Model Deployment labs! You now know how to:\n",
    "1. Create feature extractor models (EffNetB2 and ViT)\n",
    "2. Compare models across multiple metrics\n",
    "3. Build interactive Gradio demos\n",
    "4. Deploy models to Hugging Face Spaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
