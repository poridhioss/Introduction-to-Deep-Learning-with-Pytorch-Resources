{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 03: CNN & Model Comparison\n",
    "\n",
    "In this lab, we'll build a **Convolutional Neural Network (CNN)** - an architecture specifically designed for image data. We'll then compare all three models we've built across the labs.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Why CNNs are better for images\n",
    "2. Understanding Conv2d and MaxPool2d layers\n",
    "3. Building the TinyVGG architecture\n",
    "4. Training the CNN\n",
    "5. Comparing all three models\n",
    "6. Creating a confusion matrix\n",
    "7. Saving and loading the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Device\n",
    "\n",
    "We'll use CPU for this lab to keep things simple and ensure it runs on any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this lab, we'll use CPU for training\n",
    "# This keeps the lab simple and works on any machine\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = train_data.classes\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why CNNs for Images?\n",
    "\n",
    "### Problems with Fully Connected Networks for Images\n",
    "\n",
    "1. **Too many parameters**: For a 28×28 image, we need 784 inputs × hidden_units weights just for the first layer\n",
    "2. **No spatial awareness**: A pixel in the corner is treated the same as a nearby pixel\n",
    "3. **Not translation invariant**: The same object in different positions looks completely different\n",
    "\n",
    "### CNN Solution\n",
    "\n",
    "CNNs address these issues with:\n",
    "- **Convolutional layers**: Learn local patterns using small filters\n",
    "- **Parameter sharing**: Same filter is applied across the entire image\n",
    "- **Pooling layers**: Reduce spatial dimensions while preserving important features\n",
    "\n",
    "Let's explore these components!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Conv2d\n",
    "\n",
    "A convolutional layer slides a small filter (kernel) across the input image:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(\n",
    "    in_channels=1,     # Grayscale input (1 channel)\n",
    "    out_channels=10,   # Number of filters to learn\n",
    "    kernel_size=3,     # 3×3 filter\n",
    "    stride=1,          # Move 1 pixel at a time\n",
    "    padding=1          # Add 1 pixel border to preserve size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Conv2d\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Get a sample image\n",
    "sample_image = train_data[0][0].unsqueeze(0)  # Add batch dimension: [1, 1, 28, 28]\n",
    "print(f\"Input shape: {sample_image.shape}\")\n",
    "\n",
    "# Pass through conv layer\n",
    "with torch.inference_mode():\n",
    "    conv_output = conv_layer(sample_image)\n",
    "print(f\"Output shape after Conv2d: {conv_output.shape}\")\n",
    "print(f\"\\nNote: 10 feature maps, each 28x28 (size preserved due to padding=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding MaxPool2d\n",
    "\n",
    "Pooling reduces spatial dimensions by taking the maximum value in each window:\n",
    "\n",
    "```python\n",
    "nn.MaxPool2d(\n",
    "    kernel_size=2,     # 2×2 window\n",
    "    stride=2           # Move 2 pixels (reduces size by half)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate MaxPool2d\n",
    "pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(f\"Before pooling: {conv_output.shape}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    pool_output = pool_layer(conv_output)\n",
    "print(f\"After pooling: {pool_output.shape}\")\n",
    "print(f\"\\nNote: Spatial dimensions halved from 28x28 to 14x14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of convolution and pooling\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(sample_image.squeeze(), cmap='gray')\n",
    "axes[0].set_title('Original\\n(1×28×28)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# After Conv2d (show first feature map)\n",
    "axes[1].imshow(conv_output[0, 0].detach(), cmap='gray')\n",
    "axes[1].set_title('After Conv2d\\n(Feature Map 1 of 10)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# After Conv2d (show another feature map)\n",
    "axes[2].imshow(conv_output[0, 5].detach(), cmap='gray')\n",
    "axes[2].set_title('After Conv2d\\n(Feature Map 6 of 10)')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# After MaxPool2d\n",
    "axes[3].imshow(pool_output[0, 0].detach(), cmap='gray')\n",
    "axes[3].set_title('After MaxPool2d\\n(14×14)')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build the CNN Model (TinyVGG)\n",
    "\n",
    "Our CNN follows the TinyVGG architecture with three main parts. Let's understand each block before we build it.\n",
    "\n",
    "### Block 1: Extract Basic Features\n",
    "\n",
    "![Block 1](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_03/images/infra-13.svg)\n",
    "\n",
    "Block 1 is the first stage of feature extraction. It processes the raw input image and learns to detect simple patterns.\n",
    "\n",
    "**What happens in Block 1:**\n",
    "- **Input**: A 28×28 grayscale image (1 channel)\n",
    "- **First Conv2d (1→10)**: Creates 10 different 3×3 filters that learn to detect basic patterns like edges, corners, and simple textures\n",
    "- **ReLU**: Introduces non-linearity so the network can learn complex patterns (not just linear combinations)\n",
    "- **Second Conv2d (10→10)**: Further refines the detected features by combining patterns from the first layer\n",
    "- **ReLU**: Another non-linearity for more expressive power\n",
    "- **MaxPool2d**: Reduces spatial dimensions from 28×28 to 14×14, keeping only the strongest activations\n",
    "- **Output**: 10 feature maps of size 14×14\n",
    "\n",
    "### Block 2: Extract Higher-Level Patterns\n",
    "\n",
    "![Block 2](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_03/images/infra-14.svg)\n",
    "\n",
    "Block 2 builds on top of Block 1's features to detect more complex patterns like shapes and textures.\n",
    "\n",
    "**What happens in Block 2:**\n",
    "- **Input**: 10 feature maps of size 14×14 (output from Block 1)\n",
    "- **First Conv2d (10→10)**: Combines features from Block 1 to detect higher-level patterns (e.g., combining edges into shapes)\n",
    "- **ReLU**: Non-linearity for learning complex combinations\n",
    "- **Second Conv2d (10→10)**: Further combines patterns to detect even more abstract features\n",
    "- **ReLU**: Another non-linearity\n",
    "- **MaxPool2d**: Reduces spatial dimensions from 14×14 to 7×7\n",
    "- **Output**: 10 feature maps of size 7×7\n",
    "\n",
    "At this point, each of the 10 feature maps represents different high-level patterns detected in the image, at a much smaller spatial resolution.\n",
    "\n",
    "### Classifier: Make Predictions\n",
    "\n",
    "![Classifier](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_03/images/infra-15.svg)\n",
    "\n",
    "The classifier takes all the extracted features and uses them to predict which class the image belongs to.\n",
    "\n",
    "**What happens in the Classifier:**\n",
    "- **Input**: 10 feature maps of size 7×7 (total: 10 × 7 × 7 = 490 values)\n",
    "- **Flatten**: Converts the 3D tensor [10, 7, 7] into a 1D vector of 490 values\n",
    "- **Linear (490→10)**: A fully connected layer that maps the 490 features to 10 output values (one for each clothing class)\n",
    "- **Output**: 10 raw scores (logits), one per class\n",
    "\n",
    "The class with the highest score is the model's prediction. During training, these logits are passed through CrossEntropyLoss to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV2(nn.Module):\n",
    "    \"\"\"CNN model following TinyVGG architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: Input [1, 28, 28] -> Output [hidden_units, 14, 14]\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_shape,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=hidden_units,\n",
    "                out_channels=hidden_units,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
    "        )\n",
    "        \n",
    "        # Block 2: Input [hidden_units, 14, 14] -> Output [hidden_units, 7, 7]\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 14x14 -> 7x7\n",
    "        )\n",
    "        \n",
    "        # Classifier: Input [hidden_units * 7 * 7] -> Output [output_shape]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                in_features=hidden_units * 7 * 7,  # 10 * 7 * 7 = 490\n",
    "                out_features=output_shape\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_2 = FashionMNISTModelV2(\n",
    "    input_shape=1,            # Grayscale images (1 channel)\n",
    "    hidden_units=10,          # Same as other models for comparison\n",
    "    output_shape=len(class_names)  # 10 classes\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model architecture:\\n{model_2}\")\n",
    "print(f\"\\nModel is on: {next(model_2.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model with a dummy input\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    dummy_output = model_2(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define Helper Functions\n",
    "\n",
    "We'll reuse our training functions from Lab 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    return (correct / len(y_true)) * 100\n",
    "\n",
    "def print_train_time(start, end, device=None):\n",
    "    \"\"\"Print training time.\"\"\"\n",
    "    total = end - start\n",
    "    print(f\"Train time on {device}: {total:.3f} seconds\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data_loader, loss_fn, optimizer, accuracy_fn, device):\n",
    "    \"\"\"Performs one training epoch.\"\"\"\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(model, data_loader, loss_fn, accuracy_fn, device):\n",
    "    \"\"\"Evaluates model on test data.\"\"\"\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred = model(X)\n",
    "            test_loss += loss_fn(test_pred, y).item()\n",
    "            test_acc += accuracy_fn(y, test_pred.argmax(dim=1))\n",
    "    \n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train the CNN\n",
    "\n",
    "Now we'll train our CNN model on the FashionMNIST dataset. \n",
    "\n",
    "**What to expect:**\n",
    "- Training will take approximately **40-120 seconds** on CPU (depending on your machine)\n",
    "- We train for **3 epochs** - each epoch processes all 60,000 training images\n",
    "- You'll see the loss decrease and accuracy increase with each epoch\n",
    "- The model processes **1,875 batches** per epoch (60,000 images ÷ 32 batch size)\n",
    "\n",
    "**During training, you'll see:**\n",
    "- **Train loss**: How well the model fits the training data (lower is better)\n",
    "- **Train accuracy**: Percentage of training images correctly classified\n",
    "- **Test loss/accuracy**: Performance on unseen data (this is what really matters!)\n",
    "\n",
    "Be patient - CNNs have more computations per image than linear models due to the convolution operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_time_start = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch: {epoch}\\n---------\")\n",
    "    \n",
    "    train_step(\n",
    "        model=model_2,\n",
    "        data_loader=train_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    test_step(\n",
    "        model=model_2,\n",
    "        data_loader=test_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "train_time_end = timer()\n",
    "total_train_time_model_2 = print_train_time(train_time_start, train_time_end, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate and Create Model Comparison\n",
    "\n",
    "Now that training is complete, let's evaluate our CNN and compare it with the models from previous labs.\n",
    "\n",
    "We'll create an `eval_model` function that:\n",
    "- Runs the model on the entire test dataset\n",
    "- Calculates the average loss and accuracy\n",
    "- Returns the results in a dictionary for easy comparison\n",
    "\n",
    "This allows us to fairly compare V0 (baseline), V1 (non-linear), and V2 (CNN) side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, accuracy_fn, device):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y).item()\n",
    "            acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_loss\": loss / len(data_loader),\n",
    "        \"model_acc\": acc / len(data_loader)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN model\n",
    "model_2_results = eval_model(\n",
    "    model=model_2,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nCNN Model Results:\")\n",
    "print(f\"Model: {model_2_results['model_name']}\")\n",
    "print(f\"Loss: {model_2_results['model_loss']:.4f}\")\n",
    "print(f\"Accuracy: {model_2_results['model_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compare All Three Models\n",
    "\n",
    "After training all three models across our labs, let's compare their performance side by side.\n",
    "\n",
    "![Model Comparison](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_03/images/model-comparison.svg)\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "- **V2 (CNN) wins**: The CNN achieves the highest accuracy (~88%) and lowest loss (~0.33), demonstrating why CNNs are the go-to architecture for image tasks.\n",
    "\n",
    "- **V0 (Baseline) is solid**: The simple linear model achieves ~83% accuracy - a strong baseline that's fast to train (~32s).\n",
    "\n",
    "- **V1 (Non-Linear) underperforms**: Surprisingly, adding ReLU activations hurt performance (~75%). This happens because ReLU was placed after the output layer, distorting the class predictions.\n",
    "\n",
    "- **Training time vs accuracy tradeoff**: The CNN takes longer to train (~44s) but the accuracy gain is worth it for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Make Predictions with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        random_idx = torch.randint(0, len(test_data), size=[1]).item()\n",
    "        image, true_label = test_data[random_idx]\n",
    "        \n",
    "        image_device = image.unsqueeze(0).to(device)\n",
    "        pred_logits = model_2(image_device)\n",
    "        pred_label = pred_logits.argmax(dim=1).item()\n",
    "        \n",
    "        ax.imshow(image.squeeze().cpu(), cmap=\"gray\")\n",
    "        title_color = \"green\" if pred_label == true_label else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\",\n",
    "            color=title_color,\n",
    "            fontsize=10\n",
    "        )\n",
    "        ax.axis(False)\n",
    "\n",
    "plt.suptitle(\"CNN Model (V2) Predictions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Create a Confusion Matrix\n",
    "\n",
    "A confusion matrix shows where our model makes mistakes:\n",
    "- **Rows**: True labels\n",
    "- **Columns**: Predicted labels\n",
    "- **Diagonal**: Correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model_2(X)\n",
    "        y_preds.extend(y_pred.argmax(dim=1).cpu().numpy())\n",
    "        y_trues.extend(y.cpu().numpy())\n",
    "\n",
    "print(f\"Total predictions: {len(y_preds)}\")\n",
    "print(f\"Total true labels: {len(y_trues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to collect all predictions from our model on the test dataset. We loop through all test batches and store both the predicted labels and true labels in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_trues, y_preds)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - CNN Model (V2)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use scikit-learn's `confusion_matrix` and `ConfusionMatrixDisplay` to create and visualize the matrix. Each cell shows how many times a true class (row) was predicted as another class (column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most confused pairs\n",
    "import numpy as np\n",
    "\n",
    "# Zero out diagonal (correct predictions)\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "\n",
    "# Find top confusions\n",
    "print(\"Most Common Misclassifications:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for _ in range(5):\n",
    "    idx = np.unravel_index(np.argmax(cm_no_diag), cm_no_diag.shape)\n",
    "    count = cm_no_diag[idx]\n",
    "    if count > 0:\n",
    "        print(f\"{class_names[idx[0]]} mistaken as {class_names[idx[1]]}: {count} times\")\n",
    "        cm_no_diag[idx] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find which class pairs the model confuses most often. We zero out the diagonal (correct predictions) and find the highest off-diagonal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save and Load the Model\n",
    "\n",
    "Now that we have our best model, let's save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save\n",
    "MODEL_NAME = \"fashion_mnist_cnn_v2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "torch.save(obj=model_2.state_dict(), f=MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = FashionMNISTModelV2(\n",
    "    input_shape=1,\n",
    "    hidden_units=10,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Load state dict (map_location ensures it loads to CPU)\n",
    "loaded_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify loaded model works\n",
    "loaded_results = eval_model(\n",
    "    model=loaded_model,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nLoaded Model Results:\")\n",
    "print(f\"Loss: {loaded_results['model_loss']:.4f}\")\n",
    "print(f\"Accuracy: {loaded_results['model_acc']:.2f}%\")\n",
    "\n",
    "# Verify results match\n",
    "assert abs(loaded_results['model_acc'] - model_2_results['model_acc']) < 0.01, \"Results don't match!\"\n",
    "print(\"\\nResults match original model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished Across All Labs:\n",
    "\n",
    "| Lab | Model | Key Concepts | Result |\n",
    "|-----|-------|--------------|--------|\n",
    "| **Lab 01** | V0 (Baseline) | Linear layers, training loop, evaluation | ~83% |\n",
    "| **Lab 02** | V1 (Non-Linear) | ReLU, device-agnostic code, reusable functions | ~75% |\n",
    "| **Lab 03** | V2 (CNN) | Conv2d, MaxPool2d, specialized architecture | ~88% |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Architecture matters**: The CNN significantly outperformed linear models because it's designed for image data.\n",
    "\n",
    "2. **More complex ≠ better**: V1 (non-linear) actually performed *worse* than the baseline - adding complexity without proper architecture doesn't help.\n",
    "\n",
    "3. **CNNs for images**: Convolutional layers learn local patterns, pooling provides translation invariance, and parameter sharing makes training efficient.\n",
    "\n",
    "4. **Always start with a baseline**: Simple models establish performance benchmarks and help you understand when more complex models actually help.\n",
    "\n",
    "5. **Confusion matrices reveal insights**: We can see which classes are commonly confused (e.g., Shirt vs T-shirt, Pullover vs Coat).\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "From here, you could:\n",
    "- Increase hidden_units for more model capacity\n",
    "- Add more convolutional blocks for deeper features\n",
    "- Use data augmentation for better generalization\n",
    "- Try transfer learning with pre-trained models\n",
    "- Experiment with different optimizers (Adam, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
