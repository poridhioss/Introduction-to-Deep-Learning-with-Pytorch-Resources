{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: FashionMNIST & Baseline Model\n",
    "\n",
    "In this lab, we'll build our first computer vision model using PyTorch. We'll work with the FashionMNIST dataset and create a simple baseline model to classify clothing items.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Loading and exploring the FashionMNIST dataset\n",
    "2. Visualizing image data\n",
    "3. Creating DataLoaders for batch processing\n",
    "4. Building a baseline linear model\n",
    "5. Training the model\n",
    "6. Evaluating model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "Let's start by importing all the necessary libraries for this lab.\n",
    "\n",
    "### Library Explanations:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|---------|\n",
    "| **`torch`** | The core PyTorch library. Provides tensor operations, automatic differentiation, and the foundation for building neural networks. |\n",
    "| **`torch.nn`** | Neural network module containing building blocks like layers (`Linear`, `Conv2d`), loss functions (`CrossEntropyLoss`), and the base `Module` class for creating models. |\n",
    "| **`torchvision`** | PyTorch's computer vision library. Contains popular datasets, model architectures, and image transformations. |\n",
    "| **`torchvision.datasets`** | Pre-built datasets like FashionMNIST, CIFAR-10, ImageNet. Handles downloading and loading data automatically. |\n",
    "| **`torchvision.transforms.ToTensor`** | Converts PIL images or NumPy arrays to PyTorch tensors. Also scales pixel values from [0, 255] to [0.0, 1.0]. |\n",
    "| **`torch.utils.data.DataLoader`** | Wraps a dataset and provides batching, shuffling, and parallel data loading. Essential for efficient training. |\n",
    "| **`matplotlib.pyplot`** | Plotting library for visualizing images, loss curves, and other data. We use it to display sample images from the dataset. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the FashionMNIST Dataset\n",
    "\n",
    "Torchvision contains many real-world vision data like CIFAR, COCO, etc ([full list](https://docs.pytorch.org/vision/stable/datasets.html)). Fot this notebook, we will use the FashionMNIST dataset. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "![FashionMNIST](https://github.com/poridhiEng/lab-asset/blob/main/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_01/images/image-1.png?raw=true)\n",
    "\n",
    "The `ToTensor()` transform converts the images from PIL format to PyTorch tensors and scales pixel values from [0, 255] to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",           # Where to store the data\n",
    "    train=True,            # Training set\n",
    "    download=True,         # Download if not already present\n",
    "    transform=ToTensor(),  # Convert to tensor\n",
    "    target_transform=None  # No transform on labels\n",
    ")\n",
    "\n",
    "# Download test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,           # Test set\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the Dataset\n",
    "\n",
    "Let's examine the structure of our data - what does a single sample look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single sample\n",
    "image, label = train_data[0]\n",
    "\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Image dtype: {image.dtype}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Label type: {type(label)}\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Shape\n",
    "\n",
    "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
    "\n",
    "```\n",
    "[color_channels=1, height=28, width=28]\n",
    "```\n",
    "\n",
    "Having `color_channels=1` means the image is grayscale. If `color_channels=3`, the image would have pixel values for red, green and blue, this is also known as the `RGB color model`. The order of our current tensor is often referred to as `CHW` (Color Channels, Height, Width).\n",
    "\n",
    "There's debate on whether images should be represented as `CHW` (color channels first) or `HWC` (color channels last).\n",
    "\n",
    "> **Note:** You'll also see `NCHW` and `NHWC` formats where `N` stands for *number of images*. For example if you have a `batch_size=32`, your tensor shape may be `[32, 1, 28, 28]`. We'll cover batch sizes later.\n",
    "\n",
    "PyTorch generally accepts `NCHW` (channels first) as the default for many operators.\n",
    "\n",
    "### Getting Class Names\n",
    "\n",
    "FashionMNIST has 10 clothing categories. Instead of hardcoding the class names, we can get them directly from the dataset using the `.classes` attribute. This is a good practice as it:\n",
    "- Ensures consistency with the actual dataset labels\n",
    "- Makes code more reusable across different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names directly from the dataset\n",
    "# The .classes attribute returns a list of all class names in the dataset\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Images\n",
    "\n",
    "Let's visualize some samples from our dataset to get a better understanding of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single image\n",
    "image, label = train_data[0]\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")  # squeeze() removes the channel dimension for plotting\n",
    "plt.title(f\"Label: {class_names[label]}\")\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Multiple Random Samples\n",
    "\n",
    "Let's plot a grid of random images from our training data to get a better feel for what the dataset contains. This helps us understand the variety and quality of images we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple images in a 4x4 grid\n",
    "torch.manual_seed(42)  # Set seed for reproducibility\n",
    "\n",
    "# Create a figure with 4 rows and 4 columns of subplots\n",
    "fig, axes = plt.subplots(4, 4, figsize=(9, 9))\n",
    "\n",
    "# Loop through each subplot position\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    # Generate a random index to pick a sample from training data\n",
    "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "    image, label = train_data[random_idx]\n",
    "    \n",
    "    # Display the image (squeeze removes the color channel dimension for plotting)\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(class_names[label], fontsize=10)  # Show class name as title\n",
    "    ax.axis(False)  # Hide axis ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders\n",
    "\n",
    "DataLoaders help us:\n",
    "- **Batch** our data (process multiple samples at once)\n",
    "- **Shuffle** training data (prevent the model from learning order)\n",
    "- **Parallelize** data loading (faster training)\n",
    "\n",
    "We'll use a batch size of 32, which is a common starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # Shuffle training data\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # Don't shuffle test data\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of test batches: {len(test_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine a single batch\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Batch of features shape: {train_features_batch.shape}\")\n",
    "print(f\"Batch of labels shape: {train_labels_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Batch Shape\n",
    "\n",
    "The batch shape `[32, 1, 28, 28]` means:\n",
    "- **32**: Batch size (number of images)\n",
    "- **1**: Color channels\n",
    "- **28**: Height\n",
    "- **28**: Width\n",
    "\n",
    "This is the **NCHW** format (Batch, Channels, Height, Width)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the Baseline Model\n",
    "\n",
    "Now let's create our first model! We'll build a simple baseline with:\n",
    "- `nn.Flatten()`: Converts the 2D image to a 1D vector\n",
    "- `nn.Linear()`: Fully connected layers\n",
    "\n",
    "![Baseline Model Architecture](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_01/images/infra-8.svg)\n",
    "\n",
    "The diagram above shows how our baseline model processes image data. A batch of 28x28 grayscale images is first **flattened** into a 1D vector of 784 values (Input layer). This vector then passes through two **linear layers**: the first transforms 784 inputs to 10 hidden units, and the second produces 10 output values - one for each clothing category. Note that this baseline model uses **no activation functions** between layers.\n",
    "\n",
    "![FashionMNIST-Baseline-model](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_01/images/infra-1.svg)\n",
    "\n",
    "### How the Model Works\n",
    "\n",
    "Looking at the architecture diagram above, our model processes data in the following steps:\n",
    "\n",
    "1. **Input Image `[1, 28, 28]`**: A single grayscale image with 28×28 pixels (784 total pixel values)\n",
    "\n",
    "2. **Flatten Layer `[784]`**: Converts the 2D image into a 1D vector. This \"flattens\" `[1, 28, 28]` → `[784]` so it can be fed into linear layers\n",
    "\n",
    "3. **Input Layer `784`**: The flattened pixel values serve as input to the neural network\n",
    "\n",
    "4. **First Linear Layer `784 → 10`**: Takes 784 input features and transforms them to 10 hidden units. This layer learns patterns in the flattened pixel data\n",
    "\n",
    "5. **Second Linear Layer `10 → 10`**: Takes 10 hidden units and outputs 10 values (one for each clothing class)\n",
    "\n",
    "6. **Output Logits `[10]`**: Raw prediction scores for each of the 10 classes:\n",
    "   - 0: T-shirt/top\n",
    "   - 1: Trouser\n",
    "   - 2: Pullover\n",
    "   - 3: Dress\n",
    "   - 4: Coat\n",
    "   - 5: Sandal\n",
    "   - 6: Shirt\n",
    "   - 7: Sneaker\n",
    "   - 8: Bag\n",
    "   - 9: Ankle boot\n",
    "\n",
    "> **Note:** This baseline model uses **NO activation functions** between layers. This is intentional - we want to establish a simple baseline first. In Lab 02, we'll explore what happens when we add non-linearity (ReLU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "    \"\"\"Baseline model with only linear layers (no non-linearity).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten image: [1, 28, 28] -> [784]\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(\n",
    "    input_shape=28*28,          # 784 pixels\n",
    "    hidden_units=10,            # Hidden layer size\n",
    "    output_shape=len(class_names)  # 10 classes\n",
    ")\n",
    "\n",
    "print(model_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model with a Dummy Input\n",
    "\n",
    "Before training, let's verify our model works by passing a dummy input through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input (same shape as a single image)\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # [batch, channels, height, width]\n",
    "\n",
    "# Forward pass\n",
    "with torch.inference_mode():\n",
    "    dummy_output = model_0(dummy_input)\n",
    "\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {dummy_output.shape}\")\n",
    "print(f\"Output (logits): {dummy_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has shape `[1, 10]` - one row of 10 values (one for each class). These raw values are called **logits**. To get probabilities, we could apply softmax, but `nn.CrossEntropyLoss` handles this internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Loss Function and Optimizer\n",
    "\n",
    "For multi-class classification:\n",
    "- **Loss function**: `nn.CrossEntropyLoss()` - compares predicted probabilities to true labels\n",
    "- **Optimizer**: `torch.optim.SGD()` - updates weights using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    params=model_0.parameters(),\n",
    "    lr=0.1  # Learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Accuracy Function\n",
    "\n",
    "We need a way to measure how well our model is performing. Accuracy tells us the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy between true labels and predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy as a percentage\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    total = len(y_true)\n",
    "    accuracy = (correct / total) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Timing Function\n",
    "\n",
    "Let's create a helper function to track training time. This will be useful for comparing different models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float, end: float):\n",
    "    \"\"\"Print training time.\n",
    "    \n",
    "    Args:\n",
    "        start: Start time of training\n",
    "        end: End time of training\n",
    "    \n",
    "    Returns:\n",
    "        Total training time in seconds\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training the Model\n",
    "\n",
    "Now comes the exciting part - training our model! \n",
    "\n",
    "![Training Loop](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_01/images/infra-2.svg)\n",
    "\n",
    "### Understanding the Training Loop\n",
    "\n",
    "As shown in the diagram above, training happens over multiple **epochs** (complete passes through the dataset). Within each epoch, we process the data in **batches** and repeat the following steps:\n",
    "\n",
    "1. **Zero Gradients** (`optimizer.zero_grad()`): Clear gradients from the previous step to prevent accumulation\n",
    "2. **Forward Pass** (`model(features)`): Pass input data through the model to get predictions\n",
    "3. **Compute Loss** (`loss_fn(pred, target)`): Calculate how wrong our predictions are\n",
    "4. **Backward Pass** (`loss.backward()`): Compute gradients of loss with respect to model parameters\n",
    "5. **Optimizer Step** (`optimizer.step()`): Update model weights based on gradients\n",
    "\n",
    "This cycle repeats for each batch, and then moves to the next epoch.\n",
    "\n",
    "We'll train for 3 epochs (one epoch = one pass through entire training dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timer\n",
    "train_time_start = timer()\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Put model in training mode\n",
    "        model_0.train()\n",
    "        \n",
    "        # 1. Zero gradients (clear from previous step)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "        \n",
    "        # 3. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()  # Accumulate loss\n",
    "        \n",
    "        # 4. Backward pass (compute gradients)\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Optimizer step (update weights)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress every 400 batches\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "    \n",
    "    # Calculate average training loss per epoch\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # --- Testing Phase ---\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Put model in evaluation mode\n",
    "    model_0.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # Forward pass\n",
    "            test_pred = model_0(X)\n",
    "            \n",
    "            # Accumulate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y).item()\n",
    "            test_acc += accuracy_fn(\n",
    "                y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1)  # Convert logits to predictions\n",
    "            )\n",
    "    \n",
    "    # Calculate averages\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "    \n",
    "    print(f\"\\nTrain loss: {train_loss:.4f}\")\n",
    "    print(f\"Test loss: {test_loss:.4f} | Test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# End timer\n",
    "train_time_end = timer()\n",
    "\n",
    "# Print total training time\n",
    "total_train_time = print_train_time(\n",
    "    start=train_time_start,\n",
    "    end=train_time_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create Reusable Evaluation Function\n",
    "\n",
    "We already evaluated our model during training, but let's create a **reusable function** that we can use to compare multiple models in future labs. This function returns a dictionary with the model's metrics, making it easy to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: nn.Module,\n",
    "               data_loader: DataLoader,\n",
    "               loss_fn: nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Evaluate model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate\n",
    "        data_loader: DataLoader for the dataset\n",
    "        loss_fn: Loss function\n",
    "        accuracy_fn: Accuracy function\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model name, loss, and accuracy\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            loss += loss_fn(y_pred, y).item()\n",
    "            acc += accuracy_fn(\n",
    "                y_true=y,\n",
    "                y_pred=y_pred.argmax(dim=1)\n",
    "            )\n",
    "    \n",
    "    # Calculate averages\n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_loss\": loss,\n",
    "        \"model_acc\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our baseline model and store results for comparison\n",
    "model_0_results = eval_model(\n",
    "    model=model_0,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "\n",
    "# Display results (should match our training output above)\n",
    "print(f\"Baseline Model Results:\")\n",
    "print(f\"Model: {model_0_results['model_name']}\")\n",
    "print(f\"Loss: {model_0_results['model_loss']:.4f}\")\n",
    "print(f\"Accuracy: {model_0_results['model_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Make Predictions on Sample Images\n",
    "\n",
    "Let's visualize how our model performs on some random test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random samples from test data\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Get random sample\n",
    "        random_idx = torch.randint(0, len(test_data), size=[1]).item()\n",
    "        image, true_label = test_data[random_idx]\n",
    "        \n",
    "        # Make prediction\n",
    "        pred_logits = model_0(image.unsqueeze(0))  # Add batch dimension\n",
    "        pred_label = pred_logits.argmax(dim=1).item()\n",
    "        \n",
    "        # Plot\n",
    "        ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "        \n",
    "        # Color title based on correct/incorrect\n",
    "        title_color = \"green\" if pred_label == true_label else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\",\n",
    "            color=title_color,\n",
    "            fontsize=10\n",
    "        )\n",
    "        ax.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've built your first computer vision model!\n",
    "\n",
    "### What We Accomplished:\n",
    "1. **Loaded FashionMNIST** - 60,000 training + 10,000 test images\n",
    "2. **Explored the data** - 28x28 grayscale images, 10 classes\n",
    "3. **Created DataLoaders** - Batch size of 32 for efficient training\n",
    "4. **Built a baseline model** - Simple linear layers (no activation functions)\n",
    "5. **Trained for 3 epochs** - Using CrossEntropyLoss and SGD optimizer\n",
    "6. **Achieved ~80% accuracy** - Our baseline performance!\n",
    "\n",
    "### What's Next?\n",
    "In Lab 02, we'll:\n",
    "- Add **non-linear activation functions** (ReLU)\n",
    "- Explore why adding non-linearity might not always help!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
