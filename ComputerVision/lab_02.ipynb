{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Non-Linear Model & Training Functions\n",
    "\n",
    "In this lab, we'll build upon our baseline model by adding **non-linear activation functions** (ReLU) and creating **reusable training functions**.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Code from Lab 01 (data loading, DataLoaders, helper functions)\n",
    "2. Understanding the ReLU activation function\n",
    "3. Building a non-linear model\n",
    "4. Creating reusable train_step() and test_step() functions\n",
    "5. Training and comparing with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code from Lab 01\n",
    "\n",
    "Before building our new model, we need to set up the same foundation from Lab 01. The following cell contains all the essential code:\n",
    "- Import libraries\n",
    "- Load FashionMNIST dataset\n",
    "- Create DataLoaders\n",
    "- Define accuracy function\n",
    "\n",
    "Run this cell to get everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CODE FROM LAB 01 - Data Loading and Setup\n",
    "# ============================================================\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Get class names from the dataset\n",
    "class_names = train_data.classes\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nNumber of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of test batches: {len(test_dataloader)}\")\n",
    "\n",
    "# Accuracy function from Lab 01\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy between true and predicted labels.\"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    accuracy = (correct / len(y_true)) * 100\n",
    "    return accuracy\n",
    "\n",
    "# Timing function from Lab 01\n",
    "def print_train_time(start: float, end: float):\n",
    "    \"\"\"Print and return training time.\"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Non-Linearity\n",
    "\n",
    "### The Problem with Linear-Only Networks\n",
    "\n",
    "In Lab 01, our baseline model used only linear layers. The mathematical problem is that **stacking linear layers is equivalent to a single linear layer**:\n",
    "\n",
    "```\n",
    "y = W2(W1(x)) = (W2 * W1)(x) = W_combined(x)\n",
    "```\n",
    "\n",
    "This means our \"deep\" network can only learn linear relationships!\n",
    "\n",
    "### The Solution: Activation Functions\n",
    "\n",
    "By adding non-linear activation functions between layers, we allow the network to learn complex, non-linear patterns.\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "The most popular activation function is **ReLU**:\n",
    "\n",
    "```python\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "- If x > 0: output = x (passes through)\n",
    "- If x ≤ 0: output = 0 (clips negative values)\n",
    "\n",
    "Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ReLU activation function\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "relu = nn.ReLU()\n",
    "y = relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x.numpy(), y.numpy(), 'b-', linewidth=2, label='ReLU(x)')\n",
    "plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "plt.title('ReLU Activation Function: f(x) = max(0, x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ReLU with actual values\n",
    "sample_values = torch.tensor([-3, -1, 0, 1, 3], dtype=torch.float32)\n",
    "relu_output = relu(sample_values)\n",
    "\n",
    "print(\"ReLU in action:\")\n",
    "for inp, out in zip(sample_values.numpy(), relu_output.numpy()):\n",
    "    print(f\"  Input: {inp:4.1f} → ReLU → Output: {out:4.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Non-Linear Model (V1)\n",
    "\n",
    "Now let's create a new model that adds ReLU activation functions between layers.\n",
    "\n",
    "![Non-Linear Model Architecture](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_02/images/infra-7.svg)\n",
    "\n",
    "The diagram above shows how our non-linear model (V1) processes image data. A batch of 28x28 grayscale images is first **flattened** into a 1D vector of 784 values (Input layer). This vector then passes through **linear layers with ReLU activation**: after each linear transformation, the ReLU function is applied to introduce non-linearity, allowing the model to learn more complex patterns.\n",
    "\n",
    "![Model Architecture](https://raw.githubusercontent.com/poridhiEng/lab-asset/8104ff41aaf569aa65977e43cdbadc13fc1b7a34/tensorcode/Deep-learning-with-pytorch/Computer-Vision/Lab_02/images/infra-6.svg)\n",
    "\n",
    "### Key Difference from V0:\n",
    "\n",
    "**V0 (Baseline):** `Flatten → Linear → Linear`\n",
    "\n",
    "**V1 (Non-Linear):** `Flatten → Linear → ReLU → Linear → ReLU`\n",
    "\n",
    "The model processes data as follows:\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Description |\n",
    "|-------|-------------|--------------|-------------|\n",
    "| **Input** | [1, 28, 28] | - | 28×28 grayscale image |\n",
    "| **Flatten** | [1, 28, 28] | [784] | Convert 2D to 1D vector |\n",
    "| **Linear 1** | [784] | [10] | First transformation |\n",
    "| **ReLU** | [10] | [10] | Non-linear activation |\n",
    "| **Linear 2** | [10] | [10] | Output layer |\n",
    "| **ReLU** | [10] | [10] | Non-linear activation |\n",
    "| **Output** | - | [10] | Raw scores (logits) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV1(nn.Module):\n",
    "    \"\"\"Model with non-linear activation functions (ReLU).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten: [1, 28, 28] -> [784]\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),  # Non-linearity after first linear layer\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()   # Non-linearity after second linear layer\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model instance\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_1 = FashionMNISTModelV1(\n",
    "    input_shape=784,          # 28*28 pixels\n",
    "    hidden_units=10,          # Same as baseline for fair comparison\n",
    "    output_shape=len(class_names)  # 10 classes\n",
    ")\n",
    "\n",
    "print(f\"Model architecture:\\n{model_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Reusable Training Functions\n",
    "\n",
    "In Lab 01, we wrote the training loop inline. Now let's create **reusable functions** that we can use for any model.\n",
    "\n",
    "### Why functionalize the training loop?\n",
    "\n",
    "**Reusability** - Use the same functions for different models. **Cleaner code** - Main training code becomes much simpler. **Easier debugging** - Isolate and test each step. **Consistency** - Ensure all models are trained the same way.\n",
    "\n",
    "### The train_step() Function\n",
    "\n",
    "Performs a single training epoch by:\n",
    "1. Setting model to training mode\n",
    "2. Looping through all batches\n",
    "3. Forward pass → Loss → Backward pass → Optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Performs a single training epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to train\n",
    "        data_loader: DataLoader containing training data\n",
    "        loss_fn: Loss function to optimize\n",
    "        optimizer: Optimizer to update model parameters\n",
    "        accuracy_fn: Function to calculate accuracy\n",
    "    \"\"\"\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        # 3. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average loss and accuracy per epoch\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test_step() Function\n",
    "\n",
    "Evaluates model on test data by:\n",
    "1. Setting model to evaluation mode\n",
    "2. Using inference mode (no gradients)\n",
    "3. Looping through test batches\n",
    "4. Accumulating loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn):\n",
    "    \"\"\"Evaluates model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model to evaluate\n",
    "        data_loader: DataLoader containing test data\n",
    "        loss_fn: Loss function for evaluation\n",
    "        accuracy_fn: Function to calculate accuracy\n",
    "    \"\"\"\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Turn on inference mode (no gradients needed)\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss += loss_fn(test_pred, y).item()\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "    \n",
    "    # Calculate averages\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for multi-class classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer - SGD with same learning rate as baseline\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Now we can use our clean training functions. Notice how much simpler the main training loop is compared to Lab 01!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Start timing\n",
    "train_time_start = timer()\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Training loop - now much cleaner!\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch: {epoch}\\n---------\")\n",
    "    \n",
    "    # Training\n",
    "    train_step(\n",
    "        model=model_1,\n",
    "        data_loader=train_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    \n",
    "    # Testing\n",
    "    test_step(\n",
    "        model=model_1,\n",
    "        data_loader=test_dataloader,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "\n",
    "# End timing\n",
    "train_time_end = timer()\n",
    "\n",
    "# Print total training time\n",
    "total_train_time_model_1 = print_train_time(\n",
    "    start=train_time_start,\n",
    "    end=train_time_end\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Evaluation Function\n",
    "\n",
    "Let's create an evaluation function to get final metrics as a dictionary. This will be useful for comparing multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: nn.Module,\n",
    "               data_loader: DataLoader,\n",
    "               loss_fn: nn.Module,\n",
    "               accuracy_fn):\n",
    "    \"\"\"Evaluate model and return metrics as dictionary.\"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y).item()\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "    \n",
    "    loss /= len(data_loader)\n",
    "    acc /= len(data_loader)\n",
    "    \n",
    "    return {\n",
    "        \"model_name\": model.__class__.__name__,\n",
    "        \"model_loss\": loss,\n",
    "        \"model_acc\": acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the non-linear model\n",
    "model_1_results = eval_model(\n",
    "    model=model_1,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    accuracy_fn=accuracy_fn\n",
    ")\n",
    "\n",
    "print(f\"\\nNon-Linear Model Results:\")\n",
    "print(f\"Model: {model_1_results['model_name']}\")\n",
    "print(f\"Loss: {model_1_results['model_loss']:.4f}\")\n",
    "print(f\"Accuracy: {model_1_results['model_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Surprising Result!\n",
    "\n",
    "You might expect that adding non-linearity would improve performance. Let's compare with our baseline from Lab 01:\n",
    "\n",
    "| Model | Architecture | Expected | Actual |\n",
    "|-------|--------------|----------|--------|\n",
    "| **V0 (Baseline)** | Linear → Linear | Lower accuracy | ~83% |\n",
    "| **V1 (Non-Linear)** | Linear → ReLU → Linear → ReLU | Higher accuracy | ~75% |\n",
    "\n",
    "**The non-linear model performs WORSE!** Why?\n",
    "\n",
    "### Analysis\n",
    "\n",
    "**ReLU after output layer** - The final ReLU clips all negative logits to zero, which can hurt classification performance. When the model outputs a negative value for a class, that negative value carries information (the model is \"against\" that class). Clipping it to zero loses this information.\n",
    "\n",
    "**Model capacity** - With only 10 hidden units, the model is already very constrained. Adding ReLU makes it even harder to learn because ReLU zeros out negative activations, effectively reducing the model's capacity further.\n",
    "\n",
    "**Architecture mismatch** - For image data, fully connected layers aren't ideal. Images have spatial structure (nearby pixels are related), but our flatten operation destroys this structure. CNNs are much better suited for images.\n",
    "\n",
    "### Key Lesson\n",
    "\n",
    "**Adding complexity doesn't automatically improve performance!** This is why we:\n",
    "- Always start with a simple baseline\n",
    "- Add changes incrementally\n",
    "- Test each modification empirically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "torch.manual_seed(42)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "\n",
    "model_1.eval()\n",
    "with torch.inference_mode():\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        # Get random sample\n",
    "        random_idx = torch.randint(0, len(test_data), size=[1]).item()\n",
    "        image, true_label = test_data[random_idx]\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image_batch = image.unsqueeze(0)\n",
    "        \n",
    "        # Make prediction\n",
    "        pred_logits = model_1(image_batch)\n",
    "        pred_label = pred_logits.argmax(dim=1).item()\n",
    "        \n",
    "        # Plot\n",
    "        ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "        \n",
    "        # Color title based on correct/incorrect\n",
    "        title_color = \"green\" if pred_label == true_label else \"red\"\n",
    "        ax.set_title(\n",
    "            f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\",\n",
    "            color=title_color,\n",
    "            fontsize=10\n",
    "        )\n",
    "        ax.axis(False)\n",
    "\n",
    "plt.suptitle(\"Model V1 Predictions (Non-Linear)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Non-linearity is essential** for neural networks to learn complex patterns, but placement matters!\n",
    "\n",
    "2. **ReLU activation**: `max(0, x)` - simple, efficient, and widely used.\n",
    "\n",
    "3. **Reusable training functions**: `train_step()` and `test_step()` make code cleaner and more maintainable.\n",
    "\n",
    "4. **Empirical testing is crucial**: Adding non-linearity didn't help here - always test your assumptions!\n",
    "\n",
    "### Key Takeaway:\n",
    "\n",
    "The baseline model (V0) outperformed the non-linear model (V1) because:\n",
    "- ReLU after the output layer clips negative logits\n",
    "- Small hidden layer (10 units) limits capacity\n",
    "- Fully connected layers aren't optimal for image data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In Lab 03, we'll:\n",
    "- Build a **Convolutional Neural Network (CNN)**\n",
    "- Learn about **Conv2d** and **MaxPool2d** layers\n",
    "- See how architecture designed for images dramatically improves performance\n",
    "- Compare all three models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
