{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Training Loop\n",
    "\n",
    "In this notebook, we'll implement the training loop — the core algorithm that makes our model learn. By the end, our model will discover the correct `weight = 0.4` and `bias = 0.1` values from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Run this cell to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup from Lab 1\n",
    "\n",
    "Before we start training, we need to recreate everything from Lab 1: the data, model, and plotting function. Run this cell to set up the environment.\n",
    "\n",
    "![Linear Model](images/linear-model.svg)\n",
    "\n",
    "The diagram shows how our `LinearRegressionModel` works:\n",
    "- **Input X** → The model receives input features\n",
    "- **Multiply (X × weights)** → Input is multiplied by the learnable weight parameter\n",
    "- **Add (+ bias)** → The learnable bias parameter is added to the result\n",
    "- **Output y_pred** → The final prediction\n",
    "\n",
    "The model computes: `forward(X) = X × weights + bias`\n",
    "\n",
    "Currently, the model starts with **random values** for weights and bias. Our goal in this lab is to train it to discover the correct values: `weight = 0.4` and `bias = 0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Target parameters (what we want the model to learn)\n",
    "weight = 0.4\n",
    "bias = 0.1\n",
    "\n",
    "# Create data\n",
    "X = torch.arange(0, 1, 0.02).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "# Model definition\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "# Plotting function\n",
    "def plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    plt.scatter(test_data, test_labels, c=\"r\", s=4, label=\"Test data\")\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"g\", s=4, label=\"Predictions\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "# Create model instance\n",
    "model = LinearRegressionModel()\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Initial parameters: weight={model.weight.item():.4f}, bias={model.bias.item():.4f}\")\n",
    "print(f\"Target parameters:  weight={weight}, bias={bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Loss Function\n",
    "\n",
    "A **loss function** measures how wrong our predictions are. We use Mean Absolute Error (MAE), also called L1 Loss. It calculates the average absolute difference between predictions and actual values.\n",
    "\n",
    "Lower loss = better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Example: calculate loss between some predictions and targets\n",
    "example_preds = torch.tensor([0.5, 0.6, 0.7])\n",
    "example_targets = torch.tensor([0.4, 0.5, 0.8])\n",
    "example_loss = loss_fn(example_preds, example_targets)\n",
    "print(f\"Example loss: {example_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Optimizer\n",
    "\n",
    "An **optimizer** updates parameters based on gradients to reduce the loss. We use SGD (Stochastic Gradient Descent) with a learning rate of 0.01.\n",
    "\n",
    "The learning rate controls how big each update step is — too high and we overshoot, too low and training takes forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Training Loop\n",
    "\n",
    "Now we implement the 5-step training loop. This is the core algorithm that makes learning happen:\n",
    "\n",
    "1. **Zero gradients** — Clear old gradients (PyTorch accumulates them by default)\n",
    "2. **Forward pass** — Make predictions with current parameters\n",
    "3. **Calculate loss** — Measure how wrong predictions are\n",
    "4. **Backward pass** — Compute gradients (how to adjust parameters)\n",
    "5. **Update parameters** — Adjust weights using gradients\n",
    "\n",
    "We repeat this for 100 epochs (complete passes through the data).\n",
    "\n",
    "**Note on model modes:**\n",
    "- `model.train()` — Sets the model to training mode (enables dropout, batch normalization behavior for training)\n",
    "- `model.eval()` — Sets the model to evaluation mode (disables those features for inference)\n",
    "\n",
    "For our simple linear model, these don't change behavior, but it's good practice to use them consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "epoch_count = []\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # 1. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # 3. Calculate loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 4. Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "    # Record losses every 10 epochs (for smoother plotting)\n",
    "    if epoch % 10 == 0:\n",
    "        epoch_count.append(epoch)\n",
    "        train_loss_values.append(loss.detach().numpy())\n",
    "        test_loss_values.append(test_loss.detach().numpy())\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {loss:.4f} | Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss Curves\n",
    "\n",
    "Loss curves show how the model improves over time. Both train and test loss should decrease together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train Loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Test Loss Curves\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Learned vs Target Parameters\n",
    "\n",
    "Let's see how close our model got to the true weight and bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learned parameters:\")\n",
    "print(model.state_dict())\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Learned weight: {model.state_dict()['weight'].item():.4f} | Target: {weight}\")\n",
    "print(f\"  Learned bias:   {model.state_dict()['bias'].item():.4f} | Target: {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned parameters should be very close to the target values (weight=0.4, bias=0.1)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Final Predictions\n",
    "\n",
    "Now let's see how well our trained model predicts on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    final_preds = model(X_test)\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(X_train, y_train, X_test, y_test, predictions=final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green dots (predictions) should now align closely with the test data. Our model has learned the pattern!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, we:\n",
    "1. Created a **loss function** (L1Loss/MAE) to measure prediction errors\n",
    "2. Created an **optimizer** (SGD with lr=0.01) to update parameters\n",
    "3. Implemented the **5-step training loop**\n",
    "4. Trained for **100 epochs** and watched the loss decrease\n",
    "5. Verified our model learned `weight ≈ 0.4` and `bias ≈ 0.1`\n",
    "\n",
    "In **Lab 3**, we'll save this trained model to disk and load it back for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
