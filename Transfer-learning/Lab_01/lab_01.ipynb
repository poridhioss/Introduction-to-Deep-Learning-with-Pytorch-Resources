{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01: Transfer Learning Fundamentals\n",
    "\n",
    "In this notebook, we'll explore **transfer learning** — a powerful technique that allows us to leverage pretrained models for our own problems. Instead of building a CNN from scratch, we'll use a model that's already learned from millions of images.\n",
    "\n",
    "![Transfer Learning](https://raw.githubusercontent.com/poridhiEng/lab-asset/3cf35c4bc9e49c2beebb77f8f30429b9aecfb753/tensorcode/Deep-learning-with-pytorch/Transfer-learning-with-pytorch/Lab_01/images/infra-1.svg)\n",
    "\n",
    "In the diagram above, **Task 1** shows a model trained on a large dataset (like ImageNet with 1000 classes). With **transfer learning**, we take this trained model and apply it to **Task 2** — keeping the same Model 1 but replacing Head 1 with a new Head 2 for our specific problem (3 classes: pizza, steak, sushi).\n",
    "\n",
    "**Our goal**: Download a pretrained EfficientNet_B0 model, explore its architecture, freeze the base layers, and customize the classifier for our 3-class food classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required libraries. We need:\n",
    "- `torch` and `torchvision`: Core PyTorch libraries\n",
    "- `torchinfo`: To get a visual summary of our model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install torchinfo matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We need:\n",
    "- `torch`: Core PyTorch library for tensors\n",
    "- `torchvision`: For pretrained models and transforms\n",
    "- `torchinfo`: To visualize model architecture\n",
    "- `matplotlib`: For visualization\n",
    "- `os`, `zipfile`, `requests`, `pathlib`: For downloading and managing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Device\n",
    "\n",
    "Let's setup device-agnostic code. We'll use GPU if available (faster), otherwise CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Data\n",
    "\n",
    "Before we can use transfer learning, we need a dataset. We'll use a food classification dataset with **3 classes**:\n",
    "\n",
    "![Food Dataset](https://raw.githubusercontent.com/poridhiEng/lab-asset/3cf35c4bc9e49c2beebb77f8f30429b9aecfb753/tensorcode/Deep-learning-with-pytorch/Transfer-learning-with-pytorch/Lab_01/images/infra-3.svg)\n",
    "\n",
    "In this \"FoodVision Mini\" problem — classifying images of food into pizza, steak, and sushi categories. Let's download the dataset from `Poridhi's GitHub Repository` and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path to data folder\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Download pizza, steak, sushi data\n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        # Use raw.githubusercontent.com to get the actual file, not the HTML page\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/poridhioss/Introduction-to-Deep-Learning-with-Pytorch-Resources/main/Transfer-learning/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading pizza, steak, sushi data...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip pizza, steak, sushi data\n",
    "    # The zip contains train/ and test/ folders directly, so extract to image_path\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping pizza, steak, sushi data...\") \n",
    "        zip_ref.extractall(image_path)  # Extract to data/pizza_steak_sushi/\n",
    "\n",
    "    # Remove .zip file\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directory Paths\n",
    "\n",
    "Now let's create paths to our training and test directories. The data is organized in the standard image classification format:\n",
    "\n",
    "```\n",
    "data/\n",
    "└── pizza_steak_sushi/\n",
    "    ├── train/\n",
    "    │   ├── pizza/\n",
    "    │   ├── steak/\n",
    "    │   └── sushi/\n",
    "    └── test/\n",
    "        ├── pizza/\n",
    "        ├── steak/\n",
    "        └── sushi/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and test directories\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "print(f\"Training directory: {train_dir}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Transforms for Pretrained Model\n",
    "\n",
    "When using a pretrained model, it's crucial that **your data is prepared the same way as the original training data**.\n",
    "\n",
    "For models pretrained on ImageNet:\n",
    "- Images should be **224x224** pixels (minimum)\n",
    "- Values should be in range **[0, 1]** (done by ToTensor)\n",
    "- Normalized with **mean=[0.485, 0.456, 0.406]** and **std=[0.229, 0.224, 0.225]**\n",
    "\n",
    "These mean and std values were calculated from ImageNet data.\n",
    "\n",
    "### Method 1: Manual Transform Creation\n",
    "\n",
    "We can manually create the transforms using `torchvision.transforms`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transforms pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 1. Resize to 224x224\n",
    "    transforms.ToTensor(),  # 2. Convert to tensor (scales to 0-1)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 3. Normalize with ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Manual transforms created:\")\n",
    "print(manual_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Automatic Transform Creation (Recommended)\n",
    "\n",
    "Since `torchvision` v0.13+, we can automatically get the transforms used to train a pretrained model. This ensures we're using the **exact same transforms** as the original training.\n",
    "\n",
    "We access transforms through the model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a set of pretrained model weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # .DEFAULT = best available weights\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "# Get the transforms used to create our pretrained weights\n",
    "auto_transforms = weights.transforms()\n",
    "print(f\"\\nAutomatic transforms:\")\n",
    "print(auto_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how `auto_transforms` is very similar to `manual_transforms`. The benefit of automatic creation is you're guaranteed to use the same transforms as the pretrained model.\n",
    "\n",
    "We'll use `auto_transforms` for our DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Datasets and DataLoaders\n",
    "\n",
    "Now let's create our training and test datasets using `ImageFolder` and wrap them in DataLoaders.\n",
    "\n",
    "`ImageFolder` automatically:\n",
    "- Loads images from a directory structure\n",
    "- Assigns labels based on subdirectory names\n",
    "- Applies the specified transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=auto_transforms\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=auto_transforms\n",
    ")\n",
    "\n",
    "# Get class names\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Class names: {class_names}\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0  # Set to 0 for compatibility\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Number of test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Images\n",
    "\n",
    "Let's visualize a few images from our dataset to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and labels\n",
    "images, labels = next(iter(train_dataloader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")\n",
    "\n",
    "# Plot some images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    # Denormalize the image for display\n",
    "    img = images[i].permute(1, 2, 0).numpy()\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    img = img * std + mean  # Denormalize\n",
    "    img = img.clip(0, 1)  # Clip values to valid range\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(class_names[labels[i]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get and Explore a Pretrained Model\n",
    "\n",
    "Now comes the exciting part — downloading a pretrained model!\n",
    "\n",
    "We'll use **EfficientNet_B0** from `torchvision.models`. This model:\n",
    "- Has been trained on ImageNet (1.2 million images, 1000 classes)\n",
    "- Achieves ~77.7% top-1 accuracy on ImageNet\n",
    "- Has about 5.3 million parameters\n",
    "- Is a good balance of performance and efficiency\n",
    "\n",
    "### Loading the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the model with pretrained weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = torchvision.models.efficientnet_b0(weights=weights)\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"EfficientNet_B0 loaded successfully!\")\n",
    "print(f\"Model is on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model Architecture\n",
    "\n",
    "Let's print the model to see its structure. EfficientNet_B0 has three main parts:\n",
    "\n",
    "1. **features**: The convolutional backbone (feature extractor)\n",
    "2. **avgpool**: Adaptive average pooling layer\n",
    "3. **classifier**: The final classification layers\n",
    "\n",
    "Let's look at just the high-level structure first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print high-level structure (just the main components)\n",
    "print(\"EfficientNet_B0 Main Components:\")\n",
    "print(\"=\" * 50)\n",
    "for name, module in model.named_children():\n",
    "    print(f\"\\n{name}:\")\n",
    "    if name == \"features\":\n",
    "        print(f\"  Contains {len(list(module.children()))} sub-modules (convolutional blocks)\")\n",
    "    elif name == \"avgpool\":\n",
    "        print(f\"  {module}\")\n",
    "    elif name == \"classifier\":\n",
    "        print(f\"  {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a Detailed Model Summary\n",
    "\n",
    "Let's use `torchinfo.summary()` to get a detailed view of the model. This shows:\n",
    "- Input and output shapes at each layer\n",
    "- Number of parameters\n",
    "- Which layers are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a detailed summary of the model BEFORE freezing\n",
    "print(\"Model Summary (BEFORE freezing and modifying):\")\n",
    "print(\"=\" * 80)\n",
    "summary(model=model,\n",
    "        input_size=(1, 3, 224, 224),  # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "From the summary, notice:\n",
    "\n",
    "1. **Total parameters**: ~5.3 million (much more than our TinyVGG model!)\n",
    "2. **All layers are trainable**: The \"Trainable\" column shows `True` for all layers\n",
    "3. **Output shape**: The classifier outputs `[batch_size, 1000]` (for ImageNet's 1000 classes)\n",
    "\n",
    "We need to:\n",
    "1. **Freeze** the feature extractor layers (keep pretrained weights)\n",
    "2. **Modify** the classifier to output 3 classes instead of 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Freeze the Base Model Layers\n",
    "\n",
    "**Freezing** layers means keeping them unchanged during training. We want to:\n",
    "- Keep the learned patterns in the `features` section (they're already great at extracting image features)\n",
    "- Only train the `classifier` section (to learn our specific classes)\n",
    "\n",
    "To freeze layers, we set `requires_grad=False` for their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base layers in the \"features\" section\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Base layers frozen!\")\n",
    "print(f\"\\nChecking requires_grad for first layer in features:\")\n",
    "print(f\"  requires_grad = {next(model.features.parameters()).requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `features` layers won't be updated during training. This has two benefits:\n",
    "\n",
    "1. **Preserves learned patterns**: The pretrained weights stay intact\n",
    "2. **Faster training**: Fewer parameters to update means faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modify the Classifier Layer\n",
    "\n",
    "The original classifier outputs 1000 classes (for ImageNet). We only have 3 classes (pizza, steak, sushi).\n",
    "\n",
    "Let's look at the current classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the current classifier\n",
    "print(\"Current classifier:\")\n",
    "print(model.classifier)\n",
    "print(f\"\\nOutput features: {model.classifier[1].out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has:\n",
    "- **Dropout(p=0.2)**: Regularization to prevent overfitting\n",
    "- **Linear(1280, 1000)**: Maps from 1280 features to 1000 classes\n",
    "\n",
    "We'll replace this with a new classifier that outputs 3 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get the number of output classes\n",
    "output_shape = len(class_names)\n",
    "print(f\"Number of output classes: {output_shape}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Create a new classifier layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280,  # Must match the output of avgpool\n",
    "              out_features=output_shape,  # 3 classes for our problem\n",
    "              bias=True)\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nNew classifier:\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify the Modified Model\n",
    "\n",
    "Let's get another summary to see what changed. Pay attention to:\n",
    "- The **Trainable** column (most layers should be `False` now)\n",
    "- The **classifier output** (should be 3 instead of 1000)\n",
    "- The number of **trainable parameters** (should be much smaller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a detailed summary of the model AFTER freezing and modifying\n",
    "print(\"Model Summary (AFTER freezing and modifying):\")\n",
    "print(\"=\" * 80)\n",
    "summary(model=model,\n",
    "        input_size=(1, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Changes\n",
    "\n",
    "Compare before and after:\n",
    "\n",
    "| Aspect | Before | After |\n",
    "|--------|--------|-------|\n",
    "| Features layers | Trainable | **Frozen** |\n",
    "| Classifier output | 1000 | **3** |\n",
    "| Trainable params | ~5.3M | **~3,843** |\n",
    "| Non-trainable params | 0 | **~5.3M** |\n",
    "\n",
    "We went from training 5.3 million parameters to just 3,843! This means:\n",
    "- **Faster training** (fewer gradients to compute)\n",
    "- **Less memory** required\n",
    "- **Less risk of overfitting** (fewer parameters to tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model Architecture\n",
    "\n",
    "Let's verify our model works correctly by passing a sample batch through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "images, labels = next(iter(train_dataloader))\n",
    "images = images.to(device)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions (without computing gradients)\n",
    "with torch.inference_mode():\n",
    "    outputs = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {outputs.shape}\")\n",
    "print(f\"\\nExpected output shape: [batch_size, num_classes] = [{BATCH_SIZE}, {len(class_names)}]\")\n",
    "print(f\"Actual output shape matches expected: {outputs.shape == torch.Size([BATCH_SIZE, len(class_names)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the raw outputs (logits) for first 3 samples\n",
    "print(\"Raw outputs (logits) for first 3 samples:\")\n",
    "print(outputs[:3])\n",
    "\n",
    "# Convert to probabilities using softmax\n",
    "probs = torch.softmax(outputs[:3], dim=1)\n",
    "print(f\"\\nProbabilities for first 3 samples:\")\n",
    "print(probs)\n",
    "\n",
    "# Get predicted classes\n",
    "pred_classes = torch.argmax(probs, dim=1)\n",
    "print(f\"\\nPredicted classes: {pred_classes.tolist()}\")\n",
    "print(f\"Predicted labels: {[class_names[i] for i in pred_classes.tolist()]}\")\n",
    "print(f\"Actual labels: {[class_names[i] for i in labels[:3].tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is outputting predictions, but they're essentially random since we haven't trained the classifier yet. The pretrained features are providing representations, but the classifier doesn't know how to map them to our classes.\n",
    "\n",
    "**In Lab 02**, we'll train the classifier and see the accuracy improve dramatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Count Trainable vs Non-Trainable Parameters\n",
    "\n",
    "Let's explicitly count how many parameters are trainable vs frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable and non-trainable parameters\n",
    "def count_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    total = trainable + non_trainable\n",
    "    return trainable, non_trainable, total\n",
    "\n",
    "trainable, non_trainable, total = count_parameters(model)\n",
    "\n",
    "print(\"Parameter Count Summary\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Trainable parameters:     {trainable:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable:,}\")\n",
    "print(f\"Total parameters:         {total:,}\")\n",
    "print(f\"\\nPercentage trainable: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about **0.1%** of the parameters are trainable! This is the power of transfer learning — we get the benefit of 5+ million learned parameters while only needing to train ~4,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "1. **Learned about transfer learning** — using pretrained models for our own problems\n",
    "2. **Downloaded a dataset** — pizza, steak, sushi images\n",
    "3. **Created transforms** — both manual and automatic methods\n",
    "4. **Loaded a pretrained EfficientNet_B0** — ~77.7% accuracy on ImageNet\n",
    "5. **Explored the model architecture** — features, avgpool, classifier\n",
    "6. **Froze the base layers** — set `requires_grad=False`\n",
    "7. **Modified the classifier** — changed output from 1000 to 3 classes\n",
    "8. **Verified the setup** — tested that the model produces correct output shapes\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Transfer learning** lets us leverage pretrained models for new problems\n",
    "- **Freezing layers** preserves learned patterns and speeds up training\n",
    "- **Only ~3,843 trainable parameters** vs 5+ million total\n",
    "- **Data must match** the format used to train the pretrained model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Lab 02**, we'll:\n",
    "- Train our modified model on the pizza/steak/sushi dataset\n",
    "- Plot loss curves to evaluate training\n",
    "- Make predictions on test images and custom images\n",
    "- Compare results to our previous TinyVGG model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
