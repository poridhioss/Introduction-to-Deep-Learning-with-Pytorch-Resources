{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Multiclass Classification\n",
    "\n",
    "In Labs 1 and 2, we worked on **binary classification** — predicting one of two classes. But many real-world problems have more than two categories!\n",
    "\n",
    "**In this lab**, we'll extend our knowledge to **multiclass classification** — predicting one of many classes.\n",
    "\n",
    "**Examples of multiclass classification:**\n",
    "- Image classification: Cat, Dog, Bird, or Fish?\n",
    "- Digit recognition: 0, 1, 2, ..., 9?\n",
    "- Sentiment analysis: Positive, Neutral, or Negative?\n",
    "\n",
    "**Our goal**: Build a model that can classify data into one of N classes (where N > 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary vs Multiclass Classification\n",
    "\n",
    "| Aspect | Binary | Multiclass |\n",
    "|--------|--------|------------|\n",
    "| Classes | 2 | 3 or more |\n",
    "| Output | 1 value (probability of class 1) | N values (probability of each class) |\n",
    "| Output Activation | Sigmoid | Softmax |\n",
    "| Loss Function | BCEWithLogitsLoss | CrossEntropyLoss |\n",
    "| Label Type | Float (0.0 or 1.0) | Long/Int (0, 1, 2, ...) |\n",
    "| Prediction | `round(sigmoid(logits))` | `softmax(logits).argmax()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Multiclass Data\n",
    "\n",
    "We'll use `make_blobs` from scikit-learn to create clusters of points. Each cluster represents a different class.\n",
    "\n",
    "**Parameters:**\n",
    "- `n_samples=1000`: Total number of samples\n",
    "- `n_features=2`: 2D data for visualization\n",
    "- `centers=4`: 4 different classes\n",
    "- `cluster_std=1.5`: Spread of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Create multi-class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000,\n",
    "                            n_features=NUM_FEATURES,\n",
    "                            centers=NUM_CLASSES,\n",
    "                            cluster_std=1.5,\n",
    "                            random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"X shape: {X_blob.shape}\")\n",
    "print(f\"y shape: {y_blob.shape}\")\n",
    "print(f\"\\nUnique classes: {np.unique(y_blob)}\")\n",
    "print(f\"\\nFirst 5 samples:\")\n",
    "print(f\"X: {X_blob[:5]}\")\n",
    "print(f\"y: {y_blob[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tensors\n",
    "\n",
    "Note: For multiclass classification, labels should be `LongTensor` (integers), not floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)  # Long for class indices!\n",
    "\n",
    "print(f\"X dtype: {X_blob.dtype}\")\n",
    "print(f\"y dtype: {y_blob.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(\n",
    "    X_blob,\n",
    "    y_blob,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_blob_train)}\")\n",
    "print(f\"Test samples: {len(X_blob_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data\n",
    "\n",
    "Let's plot our 4-class data. Each color represents a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu, s=40)\n",
    "plt.title(f\"Multiclass Classification Data ({NUM_CLASSES} classes)\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.colorbar(label=\"Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Multiclass Classification Model\n",
    "\n",
    "Our model is similar to Lab 2, but with a key difference in the output layer:\n",
    "\n",
    "**Binary (Lab 2):** Output = 1 (probability of being class 1)  \n",
    "**Multiclass:** Output = NUM_CLASSES (probability for each class)\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input: 2 features\n",
    "- Hidden: 8 neurons (x2 layers)\n",
    "- Output: 4 values (one per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Multiclass classification model.\n",
    "        \n",
    "        Args:\n",
    "            input_features: Number of input features (2 for our 2D data)\n",
    "            output_features: Number of output classes (4 for our data)\n",
    "            hidden_units: Neurons in hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create model instance\n",
    "model = BlobModel(input_features=NUM_FEATURES, \n",
    "                  output_features=NUM_CLASSES, \n",
    "                  hidden_units=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Function and Optimizer\n",
    "\n",
    "For multiclass classification, we use:\n",
    "\n",
    "**Loss: `nn.CrossEntropyLoss()`**\n",
    "- Combines softmax and negative log likelihood\n",
    "- Works with raw logits (no need to apply softmax manually)\n",
    "- Expects class indices (0, 1, 2, 3) not one-hot encoded\n",
    "\n",
    "**Optimizer: SGD**\n",
    "- Standard gradient descent with learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Softmax and Model Outputs\n",
    "\n",
    "Before training, let's understand how multiclass predictions work.\n",
    "\n",
    "**The pipeline:**\n",
    "1. **Logits**: Raw model output (can be any value)\n",
    "2. **Softmax**: Converts to probabilities (0-1, sum to 1)\n",
    "3. **Argmax**: Picks the class with highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the untrained model outputs\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model(X_blob_test[:5])\n",
    "\n",
    "print(\"Raw logits (model output):\")\n",
    "print(y_logits)\n",
    "print(f\"\\nShape: {y_logits.shape} (5 samples, 4 classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax to get probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "\n",
    "print(\"Prediction probabilities (after softmax):\")\n",
    "print(y_pred_probs)\n",
    "print(f\"\\nSum of probabilities per sample: {y_pred_probs.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each row sums to 1.0 — that's what softmax does! It converts logits into a probability distribution over classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted class using argmax\n",
    "y_preds = y_pred_probs.argmax(dim=1)\n",
    "\n",
    "print(\"Predicted classes (argmax):\")\n",
    "print(y_preds)\n",
    "\n",
    "print(\"\\nActual classes:\")\n",
    "print(y_blob_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Example\n",
    "\n",
    "Let's look at one sample in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed look at first sample\n",
    "print(\"First sample probabilities:\")\n",
    "for i, prob in enumerate(y_pred_probs[0]):\n",
    "    print(f\"  Class {i}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPredicted class: {torch.argmax(y_pred_probs[0]).item()}\")\n",
    "print(f\"Actual class: {y_blob_test[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "The training loop is similar to previous labs, with one key difference in how we make predictions:\n",
    "\n",
    "**Binary:** `y_pred = torch.round(torch.sigmoid(logits))`\n",
    "**Multiclass:** `y_pred = torch.softmax(logits, dim=1).argmax(dim=1)`\n",
    "\n",
    "The training loop follows these 5 steps:\n",
    "\n",
    "1. Zero gradients\n",
    "2. Forward pass\n",
    "3. Calculate loss\n",
    "4. Backward pass (backpropagation)\n",
    "5. Optimizer step\n",
    "\n",
    "![Neural Network Training Flow](https://raw.githubusercontent.com/poridhiEng/lab-asset/180b5d3f8ff55ed46357e14dce40bde6ae94645d/tensorcode/Deep-learning-with-pytorch/Classification/Lab_03/images/infra-8.svg)\n",
    "\n",
    "The diagram above illustrates the complete training pipeline for our multiclass classifier. Data flows forward through multiple hidden layers with ReLU activations, producing 4 outputs (one per class). The CrossEntropyLoss computes the error, gradients flow backward through the network, and SGD updates the weights. This cycle repeats each epoch until the model converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model.train()\n",
    "\n",
    "    # 1. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Forward pass\n",
    "    y_logits = model(X_blob_train)  # model outputs raw logits\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # logits -> probs -> labels\n",
    "\n",
    "    # 3. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_blob_train)  # CrossEntropyLoss expects raw logits\n",
    "    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)\n",
    "\n",
    "    # 4. Backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(X_blob_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "        test_loss = loss_fn(test_logits, y_blob_test)\n",
    "        test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Model\n",
    "\n",
    "Let's make final predictions and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model(X_blob_test)\n",
    "\n",
    "# Convert logits to predictions\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "y_preds = y_pred_probs.argmax(dim=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "final_acc = accuracy_fn(y_true=y_blob_test, y_pred=y_preds)\n",
    "print(f\"Final Test Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "# Show some predictions\n",
    "print(f\"\\nFirst 10 predictions: {y_preds[:10].tolist()}\")\n",
    "print(f\"First 10 actual:      {y_blob_test[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Decision Boundaries\n",
    "\n",
    "Let's see how our model divides the feature space into 4 regions, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_multiclass(model, X, y):\n",
    "    \"\"\"Plots decision boundaries for multiclass classification.\"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Multiclass: use softmax + argmax\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary_multiclass(model, X_blob_train, y_blob_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary_multiclass(model, X_blob_test, y_blob_test)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has learned to divide the space into 4 regions, with each region corresponding to one class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding the Predictions in Detail\n",
    "\n",
    "Let's look at a few samples to understand how the model makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for a few samples\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    sample_logits = model(X_blob_test[:3])\n",
    "    sample_probs = torch.softmax(sample_logits, dim=1)\n",
    "    sample_preds = sample_probs.argmax(dim=1)\n",
    "\n",
    "# Display detailed predictions\n",
    "for i in range(3):\n",
    "    print(f\"\\n=== Sample {i+1} ===\")\n",
    "    print(f\"Features: X1={X_blob_test[i][0]:.2f}, X2={X_blob_test[i][1]:.2f}\")\n",
    "    print(f\"Actual class: {y_blob_test[i].item()}\")\n",
    "    print(f\"Predicted class: {sample_preds[i].item()}\")\n",
    "    print(\"Probabilities:\")\n",
    "    for j in range(NUM_CLASSES):\n",
    "        bar = \"█\" * int(sample_probs[i][j] * 20)\n",
    "        print(f\"  Class {j}: {sample_probs[i][j]:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experimenting with More Classes\n",
    "\n",
    "Let's try with more classes to see how the model adapts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with 6 classes\n",
    "NUM_CLASSES_NEW = 6\n",
    "\n",
    "X_new, y_new = make_blobs(n_samples=1000,\n",
    "                          n_features=2,\n",
    "                          centers=NUM_CLASSES_NEW,\n",
    "                          cluster_std=1.2,\n",
    "                          random_state=42)\n",
    "\n",
    "X_new = torch.from_numpy(X_new).type(torch.float)\n",
    "y_new = torch.from_numpy(y_new).type(torch.LongTensor)\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "model_new = BlobModel(input_features=2, output_features=NUM_CLASSES_NEW, hidden_units=8)\n",
    "optimizer_new = torch.optim.SGD(model_new.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for epoch in range(100):\n",
    "    model_new.train()\n",
    "    y_logits = model_new(X_train_new)\n",
    "    loss = loss_fn(y_logits, y_train_new)\n",
    "    optimizer_new.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_new.step()\n",
    "\n",
    "# Evaluate\n",
    "model_new.eval()\n",
    "with torch.inference_mode():\n",
    "    test_logits = model_new(X_test_new)\n",
    "    test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "    test_acc = accuracy_fn(y_test_new, test_preds)\n",
    "\n",
    "print(f\"6-Class Model Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 6-class decision boundary\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(f\"6-Class Classification (Accuracy: {test_acc:.1f}%)\")\n",
    "plot_decision_boundary_multiclass(model_new, X_test_new, y_test_new)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model architecture works for any number of classes — just change the output layer size!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Congratulations on completing Lab 3 and the entire classification series!\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "In this lab, we extended our binary classification knowledge to **multiclass classification**:\n",
    "\n",
    "1. **Created multiclass data** using `make_blobs` with 4 classes\n",
    "2. **Built a multiclass model** with output size = number of classes\n",
    "3. **Used CrossEntropyLoss** for multiclass classification\n",
    "4. **Applied softmax** to convert logits to probabilities\n",
    "5. **Used argmax** to get the predicted class\n",
    "6. **Visualized decision boundaries** showing multiple regions\n",
    "7. **Experimented with 6 classes** to show flexibility\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Multiclass classification** predicts one of N classes (N > 2)\n",
    "2. **Softmax** converts logits to probabilities that sum to 1\n",
    "3. **CrossEntropyLoss** combines softmax and negative log likelihood\n",
    "4. **Argmax** selects the class with highest probability\n",
    "5. The same model architecture scales to any number of classes\n",
    "\n",
    "### Project Complete!\n",
    "\n",
    "Congratulations! You've completed all 3 classification labs:\n",
    "- **Lab 1**: Binary classification basics (linear model fails)\n",
    "- **Lab 2**: Adding ReLU for non-linear patterns (high accuracy)\n",
    "- **Lab 3**: Multiclass classification with softmax\n",
    "\n",
    "You now have a solid foundation in PyTorch classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
