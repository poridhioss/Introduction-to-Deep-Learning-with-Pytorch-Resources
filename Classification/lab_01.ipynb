{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Binary Classification - Data Preparation and Basic Model\n",
    "\n",
    "In this notebook, we'll create non-linear classification data (circles), split it into training and test sets, visualize it, and build our first classification model in PyTorch.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We have data points distributed in two concentric circles. Given a new point's coordinates (X1, X2), our goal is to predict which circle it belongs to — the **inner circle (Class 0)** or the **outer circle (Class 1)**.\n",
    "\n",
    "![Binary Classification Problem](https://raw.githubusercontent.com/poridhiEng/lab-asset/c86bc88e676d50722669abf52c9c25213adc5b70/tensorcode/Deep-learning-with-pytorch/Classification/Lab_01/images/circles-problem.svg)\n",
    "\n",
    "This is a **binary classification** problem — we predict one of two possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "First, let's install the required libraries by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We need:\n",
    "- `torch`: Core PyTorch library for tensors and neural networks\n",
    "- `sklearn.datasets`: To generate classification data (circles)\n",
    "- `matplotlib`: For visualization\n",
    "- `pandas`: For viewing data in tabular format\n",
    "- `sklearn.model_selection`: To split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Classification Data\n",
    "\n",
    "Unlike linear regression where we predicted continuous values, **classification** predicts discrete classes (categories).\n",
    "\n",
    "We'll use `make_circles` from scikit-learn to create two concentric circles:\n",
    "- **Inner circle**: Class 0 (red points)\n",
    "- **Outer circle**: Class 1 (blue points)\n",
    "\n",
    "The data is intentionally **non-linear** — you cannot draw a straight line to separate the two classes. This will be important later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples,\n",
    "                    noise=0.03,  # a little bit of noise to the dots\n",
    "                    random_state=42)  # keep random state so we get the same values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # Features: 2D coordinates\n",
    "print(f\"y shape: {y.shape}\")  # Labels: 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data\n",
    "\n",
    "Let's view the first few samples to understand our data structure. Each sample has:\n",
    "- **X**: Two features (X1, X2) representing coordinates on a 2D plane\n",
    "- **y**: A label (0 or 1) indicating which circle the point belongs to\n",
    "\n",
    "**How are labels decided?**\n",
    "\n",
    "The `make_circles` function generates two concentric circles with specific radii:\n",
    "- **Outer circle** (Class 1): radius = **1.0**\n",
    "- **Inner circle** (Class 0): radius = **0.8** (default `factor=0.8`)\n",
    "\n",
    "The label is based on which circle the point was generated on:\n",
    "- If `distance from center ≈ 0.8` → **Class 0** (inner)\n",
    "- If `distance from center ≈ 1.0` → **Class 1** (outer)\n",
    "\n",
    "Where distance = `√(X1² + X2²)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 X features:\\n{X[:5]}\")\n",
    "print(f\"\\nFirst 5 y labels:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Data as a DataFrame\n",
    "\n",
    "Let's put our data into a pandas DataFrame to see it in a nice tabular format. Each row represents one sample with its X1, X2 coordinates and the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of circle data\n",
    "circles = pd.DataFrame({\"X1\": X[:, 0],\n",
    "                        \"X2\": X[:, 1],\n",
    "                        \"label\": y})\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Class Balance\n",
    "\n",
    "It's important to check if our classes are balanced. An imbalanced dataset (e.g., 90% class 0, 10% class 1) can cause problems during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check different labels\n",
    "circles.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 samples in each class — perfectly balanced!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Data\n",
    "\n",
    "Let's plot our data to see the two circles. The color represents the class:\n",
    "- **Blue**: Class 0 (inner circle, radius ≈ 0.8)\n",
    "- **Red**: Class 1 (outer circle, radius ≈ 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    c=y,\n",
    "    cmap=plt.cm.RdYlBu_r  # reversed colormap\n",
    ")\n",
    "plt.title(\"Circle Classification Data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Input and Output Shapes and Data Type\n",
    "\n",
    "Before building a model, we need to understand our data's shape:\n",
    "- **Input (X)**: 1000 samples, each with 2 features → shape `[1000, 2]`\n",
    "- **Output (y)**: 1000 labels, each is 0 or 1 → shape `[1000]`\n",
    "\n",
    "This tells us our model needs:\n",
    "- **Input layer**: 2 features (X1, X2)\n",
    "- **Output layer**: 1 value (probability of being class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of our features and labels\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data type - currently NumPy arrays\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels are also NumPy arrays - need to convert to tensors\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Turn Data into Tensors and Split\n",
    "\n",
    "PyTorch works with tensors, not NumPy arrays. We need to:\n",
    "1. Convert X and y from NumPy arrays to PyTorch tensors\n",
    "2. Split into training (80%) and test (20%) sets\n",
    "\n",
    "We use `torch.float` for X (features) because neural networks work with floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# View the first five samples\n",
    "print(\"First 5 X samples:\")\n",
    "print(X[:5])\n",
    "print(\"\\nFirst 5 y labels:\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2,  # 20% test, 80% train\n",
    "                                                    random_state=42)  # reproducible split\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 800 samples for training and 200 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Classification Model\n",
    "\n",
    "Now let's build our first classification model. It's similar to linear regression, but designed for classification:\n",
    "\n",
    "**Model Architecture:**\n",
    "- **Input Layer**: Takes 2 features (X1, X2)\n",
    "- **Hidden Layer**: 5 neurons (learns patterns)\n",
    "- **Output Layer**: 1 neuron (predicts class probability)\n",
    "\n",
    "![Neural Network Architecture](https://raw.githubusercontent.com/poridhiEng/lab-asset/c86bc88e676d50722669abf52c9c25213adc5b70/tensorcode/Deep-learning-with-pytorch/Classification/Lab_01/images/network.svg)\n",
    "\n",
    "The diagram above shows our network structure: 2 input neurons receive the coordinates (X1, X2), 5 hidden neurons learn to detect patterns in the data, and 1 output neuron produces the class prediction.\n",
    "\n",
    "**Code Breakdown:**\n",
    "- `nn.Module`: Base class for all PyTorch models\n",
    "- `__init__`: Defines the layers (what the model has)\n",
    "- `forward`: Defines how data flows through the layers (what the model does)\n",
    "- `nn.Linear(in, out)`: A fully connected layer that transforms `in` features to `out` features\n",
    "\n",
    "This is a simple neural network with only **linear layers** (no activation functions between layers). We'll see why this is a problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)  # takes in 2 features, produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)  # takes in 5 features, produces 1 feature\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Return the output of layer_2, a single feature, the same shape as y\n",
    "        return self.layer_2(self.layer_1(x))  # x -> layer_1 -> layer_2 -> output\n",
    "\n",
    "# Create an instance of the model\n",
    "model_0 = CircleModelV0()\n",
    "print(model_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using nn.Sequential\n",
    "\n",
    "We can also build the same model using `nn.Sequential`, which is more concise.\n",
    "\n",
    "`nn.Sequential` performs a forward pass computation of the input data through the layers **in the order they appear**. So the data flows: Input → Layer 1 → Layer 2 → Output.\n",
    "\n",
    "**When to use which?**\n",
    "- `nn.Sequential`: Quick and simple when layers just stack one after another\n",
    "- `nn.Module` class: When you need more control over how data flows through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate CircleModelV0 with nn.Sequential\n",
    "model_0_sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ")\n",
    "\n",
    "print(model_0_sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Up Loss Function and Optimizer\n",
    "\n",
    "For binary classification, we use:\n",
    "\n",
    "**Loss Function: `nn.BCEWithLogitsLoss()`**\n",
    "- BCE = Binary Cross Entropy\n",
    "- \"WithLogits\" means it applies sigmoid internally\n",
    "- Measures how wrong our predictions are for binary classification\n",
    "\n",
    "**Why `BCEWithLogitsLoss` instead of `BCELoss`?**\n",
    "- `nn.BCELoss()`: Expects probabilities (0-1), so you must apply sigmoid yourself\n",
    "- `nn.BCEWithLogitsLoss()`: Has sigmoid built-in, takes raw logits directly\n",
    "\n",
    "Using `BCEWithLogitsLoss` is recommended because it's more numerically stable and convenient.\n",
    "\n",
    "**BCE Formula:**\n",
    "$$\\text{BCE} = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
    "\n",
    "Where:\n",
    "- **y** = target label (0 or 1)\n",
    "- **ŷ** = predicted probability (between 0 and 1)\n",
    "\n",
    "When the model is confident and correct → **low loss**. When the model is wrong or uncertain → **high loss**.\n",
    "\n",
    "**Optimizer: `SGD` (Stochastic Gradient Descent)**\n",
    "- Updates model parameters to reduce loss\n",
    "- `lr=0.1` is the learning rate (how big the update steps are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Function\n",
    "\n",
    "We also need a way to measure how accurate our predictions are. Accuracy = (correct predictions / total predictions) × 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()  # count matching predictions\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Model Outputs: Logits → Probabilities → Labels\n",
    "\n",
    "Our model outputs **raw logits** (unbounded numbers). We need to convert these to class predictions:\n",
    "\n",
    "1. **Logits** (raw output) → can be any number (-∞ to +∞)\n",
    "2. **Probabilities** (after sigmoid) → between 0 and 1\n",
    "3. **Labels** (after rounding) → 0 or 1\n",
    "\n",
    "Let's see this in action with our untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 outputs of the forward pass on the test data\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_0(X_test)[:5]\n",
    "\n",
    "print(\"Raw logits (model output):\")\n",
    "print(y_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sigmoid on model logits to get prediction probabilities\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "\n",
    "print(\"Prediction probabilities (after sigmoid):\")\n",
    "print(y_pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted labels (round the prediction probabilities)\n",
    "y_preds = torch.round(y_pred_probs)\n",
    "\n",
    "print(\"Predicted labels (after rounding):\")\n",
    "print(y_preds.squeeze())\n",
    "\n",
    "print(\"\\nActual labels:\")\n",
    "print(y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Model\n",
    "\n",
    "Now let's train our model! The training loop is the heart of machine learning. Every training iteration follows these 5 steps:\n",
    "\n",
    "1. **Zero Gradients** — Clear gradients from previous iteration\n",
    "   ```python\n",
    "   optimizer.zero_grad()\n",
    "   ```\n",
    "   PyTorch accumulates gradients by default. If we don't clear them, gradients from previous iterations would add up incorrectly.\n",
    "\n",
    "2. **Forward Pass** — Make predictions using current parameters\n",
    "   ```python\n",
    "   y_pred = model(X_train)\n",
    "   ```\n",
    "\n",
    "3. **Calculate Loss** — Measure how wrong the predictions are\n",
    "   ```python\n",
    "   loss = loss_fn(y_pred, y_train)\n",
    "   ```\n",
    "\n",
    "4. **Backward Pass** — Compute gradients (how to adjust parameters)\n",
    "   ```python\n",
    "   loss.backward()\n",
    "   ```\n",
    "\n",
    "5. **Update Parameters** — Adjust weights using gradients\n",
    "   ```python\n",
    "   optimizer.step()\n",
    "   ```\n",
    "\n",
    "We'll train for 100 epochs and print progress every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 2. Forward pass (model outputs raw logits)\n",
    "    y_logits = model_0(X_train).squeeze()  # squeeze to remove extra dimensions\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # logits -> probs -> labels\n",
    "  \n",
    "    # 3. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train)  # BCEWithLogitsLoss works with raw logits\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 4. Backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Forward pass\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # Calculate loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model\n",
    "\n",
    "Let's make predictions with our trained model and see how it performs.\n",
    "\n",
    "**Something's wrong!** Notice the accuracy is around 50% — that's basically random guessing for a two-class problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = torch.round(torch.sigmoid(model_0(X_test))).squeeze()\n",
    "\n",
    "# Calculate final accuracy\n",
    "final_acc = accuracy_fn(y_true=y_test, y_pred=y_preds)\n",
    "print(f\"Final Test Accuracy: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizing Decision Boundaries\n",
    "\n",
    "To understand why our model is performing poorly, let's visualize its **decision boundary** — the line/region where the model switches from predicting class 0 to class 1.\n",
    "\n",
    "**How the `plot_decision_boundary` function works:**\n",
    "\n",
    "1. **Create a grid of points**: We generate a mesh of points covering the entire plot area using `np.meshgrid`. This creates 101×101 = 10,201 points.\n",
    "\n",
    "2. **Make predictions on the grid**: We pass all grid points through the model to get predictions for every location in the 2D space.\n",
    "\n",
    "3. **Color the regions**: Using `plt.contourf`, we color each region based on what the model would predict for points in that area:\n",
    "   - Blue region → Model predicts Class 0\n",
    "   - Red region → Model predicts Class 1\n",
    "\n",
    "4. **Overlay the actual data**: We scatter plot the real data points on top, so we can see how well the colored regions match the actual class distribution.\n",
    "\n",
    "The boundary between the colored regions shows where the model's prediction changes from one class to another — this is the **decision boundary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\"\"\"\n",
    "    # Put everything to CPU\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    # Setup prediction boundaries and grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    # Make features\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    # Binary classification\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    # Reshape preds and plot\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Why Is The Model Failing?\n",
    "\n",
    "Look at the decision boundary — **it's a straight line!**\n",
    "\n",
    "Our model is trying to separate circles with a straight line, which is impossible. The model can only learn **linear patterns** because it only contains linear layers.\n",
    "\n",
    "### The Problem: Linear Model for Non-Linear Data\n",
    "\n",
    "Our circle data requires a **circular/curved decision boundary**, but our model can only create **straight lines**.\n",
    "\n",
    "**Think about it**: No matter how you rotate or move a straight line, you can never perfectly separate the inner circle from the outer circle.\n",
    "\n",
    "### The Solution (Coming in Lab 2)\n",
    "\n",
    "To solve non-linear problems, we need **non-linear activation functions** like ReLU (Rectified Linear Unit). These allow our model to learn curved decision boundaries.\n",
    "\n",
    "In Lab 2, we'll add ReLU activation functions and watch our model's accuracy improve significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we:\n",
    "\n",
    "1. **Created classification data** using `make_circles` — 1000 samples in two concentric circles\n",
    "2. **Explored the data** using pandas and visualizations\n",
    "3. **Split the data** into training (800 samples) and test (200 samples) sets\n",
    "4. **Built a classification model** with two linear layers\n",
    "5. **Trained the model** for 100 epochs\n",
    "6. **Evaluated the model** — accuracy was only ~50% (random guessing!)\n",
    "7. **Visualized decision boundaries** — the model can only draw straight lines\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Linear models cannot solve non-linear problems.** Our model failed because it could only create straight-line decision boundaries, but our data requires circular boundaries.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Lab 2**, we'll add non-linear activation functions (ReLU) to our model, enabling it to learn complex patterns and achieve much better accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
