{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Binary Classification with Non-Linear Activation (ReLU)\n",
    "\n",
    "In Lab 1, we built a simple classification model that could only achieve ~50% accuracy on our circle data. The problem: our model could only draw straight-line decision boundaries.\n",
    "\n",
    "**In this lab**, we'll add **non-linear activation functions (ReLU)** to our model, enabling it to learn curved decision boundaries and achieve much better accuracy!\n",
    "\n",
    "**Our goal**: Improve accuracy from ~50% to a much higher level by adding non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup from Lab 1\n",
    "\n",
    "First, let's set up everything we need — installing dependencies, importing libraries, and recreating our circle data.\n",
    "\n",
    "**From Lab 1, we're using:**\n",
    "- Circle data generation with `make_circles`\n",
    "- Train/test split\n",
    "- Accuracy function\n",
    "- Decision boundary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recreating the Circle Data\n",
    "\n",
    "Let's recreate the same circle data from Lab 1. This ensures we're comparing the same problem with different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and plot data\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_circles(n_samples=1000,\n",
    "                    noise=0.03,\n",
    "                    random_state=42)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu)\n",
    "plt.title(\"Circle Classification Data\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors and split into train and test sets\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nFirst 5 X_train samples:\\n{X_train[:5]}\")\n",
    "print(f\"\\nFirst 5 y_train labels: {y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Let's define our accuracy function and decision boundary plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\"\"\"\n",
    "    model.to(\"cpu\")\n",
    "    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n",
    "\n",
    "    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        y_logits = model(X_to_pred_on)\n",
    "\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "\n",
    "    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n",
    "    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Problem: Why Did Our Linear Model Fail?\n",
    "\n",
    "In Lab 1, we built a neural network with only linear layers:\n",
    "\n",
    "![Linear Neural Network](https://raw.githubusercontent.com/poridhiEng/lab-asset/c86bc88e676d50722669abf52c9c25213adc5b70/tensorcode/Deep-learning-with-pytorch/Classification/Lab_02/images/network.svg)\n",
    "\n",
    "The network had:\n",
    "- **2 input neurons** (X1, X2 coordinates)\n",
    "- **5 hidden neurons** (to learn patterns)\n",
    "- **1 output neuron** (class prediction)\n",
    "\n",
    "But even with this architecture, we only achieved ~50% accuracy. Why?\n",
    "\n",
    "### The Math Problem\n",
    "\n",
    "Our model used only linear transformations:\n",
    "\n",
    "```python\n",
    "layer_1 = nn.Linear(2, 5)  # Linear transformation\n",
    "layer_2 = nn.Linear(5, 1)  # Another linear transformation\n",
    "```\n",
    "\n",
    "**Stacking linear transformations just gives you another linear transformation!**\n",
    "\n",
    "```\n",
    "y = W2 * (W1 * x + b1) + b2\n",
    "y = W2*W1*x + W2*b1 + b2\n",
    "y = W'*x + b'  # Still linear!\n",
    "```\n",
    "\n",
    "### The Result: A Straight Line Decision Boundary\n",
    "\n",
    "No matter how many linear layers we stack, the model can only draw **straight lines**:\n",
    "\n",
    "![Linear Decision Boundary](https://github.com/poridhiEng/lab-asset/blob/main/tensorcode/Deep-learning-with-pytorch/Classification/Lab_02/images/image-1.png?raw=true)\n",
    "\n",
    "The plots show the decision boundary from Lab 1:\n",
    "- **Red/Pink region**: Model predicts Class 0\n",
    "- **Blue region**: Model predicts Class 1\n",
    "- **The boundary is a straight line** — it cannot curve to separate the circles!\n",
    "\n",
    "Our circle data needs **curved boundaries**, but a linear model can never create them. That's why we got only ~50% accuracy (random guessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Solution: Non-Linear Activation Functions\n",
    "\n",
    "To learn non-linear patterns, we add **activation functions** between layers. The most popular one is **ReLU (Rectified Linear Unit)**:\n",
    "\n",
    "```\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "\n",
    "- If x > 0: output = x\n",
    "- If x ≤ 0: output = 0\n",
    "\n",
    "### Why Does ReLU Help?\n",
    "\n",
    "ReLU introduces non-linearity by \"bending\" the output. When combined with multiple layers, the model can approximate **any** function, including curves and circles!\n",
    "\n",
    "Let's visualize what ReLU does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy tensor to visualize ReLU\n",
    "A = torch.arange(-10, 10, 1, dtype=torch.float32)\n",
    "print(f\"Input tensor: {A}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the input (straight line)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(A, label=\"Input\")\n",
    "plt.title(\"Input: Straight Line\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Apply ReLU\n",
    "relu = nn.ReLU()\n",
    "A_relu = relu(A)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(A_relu, label=\"After ReLU\", color=\"orange\")\n",
    "plt.title(\"After ReLU: Non-Linear!\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how ReLU turns all negative values to 0, creating a \"bend\" in the line. This simple operation allows neural networks to learn complex patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a Model with Non-Linearity\n",
    "\n",
    "Now let's build a model that uses ReLU activation functions between layers.\n",
    "\n",
    "![Neural Network with ReLU Activation](https://raw.githubusercontent.com/poridhiEng/lab-asset/c86bc88e676d50722669abf52c9c25213adc5b70/tensorcode/Deep-learning-with-pytorch/Classification/Lab_02/images/relu-network.svg)\n",
    "\n",
    "The diagram shows our new architecture with ReLU activations:\n",
    "\n",
    "- **Input Layer (X₁, X₂)**: Takes the 2D coordinates of each point\n",
    "- **Layer 1 → ReLU**: First linear transformation followed by ReLU activation\n",
    "- **Layer 2 → ReLU**: Second linear transformation followed by ReLU activation  \n",
    "- **Layer 3 → Output**: Final linear transformation producing the prediction\n",
    "\n",
    "**The key difference from Lab 1**: After each hidden layer, we apply ReLU which introduces non-linearity. This allows the network to learn curved decision boundaries instead of just straight lines.\n",
    "\n",
    "**Model Architecture:**\n",
    "- Input: 2 features (X1, X2)\n",
    "- Layer 1: Linear(2 → 10) + ReLU\n",
    "- Layer 2: Linear(10 → 10) + ReLU\n",
    "- Layer 3: Linear(10 → 1) (output)\n",
    "\n",
    "The ReLU between layers allows the model to learn non-linear patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with non-linear activation function\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU()  # <- The key addition!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Intersperse ReLU activation function between layers\n",
    "        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "model_2 = CircleModelV2()\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Flow Through the Model\n",
    "\n",
    "Here's how data flows through our model step by step:\n",
    "\n",
    "**Input [2] → Layer 1 [10] → ReLU → Layer 2 [10] → ReLU → Layer 3 [1] → Output**\n",
    "\n",
    "1. **Input (X₁, X₂)**: The 2D coordinates enter the network\n",
    "2. **Layer 1**: Linear transformation from 2 → 10 neurons\n",
    "3. **ReLU**: Apply `max(0, x)` — introduces non-linearity\n",
    "4. **Layer 2**: Linear transformation from 10 → 10 neurons\n",
    "5. **ReLU**: Apply `max(0, x)` again — more non-linearity\n",
    "6. **Layer 3**: Linear transformation from 10 → 1 neuron\n",
    "7. **Output**: Raw logits (will be passed through sigmoid for prediction)\n",
    "\n",
    "The ReLU activations allow each layer to learn different non-linear transformations, combining to create complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setting Up Loss and Optimizer\n",
    "\n",
    "We'll use the same loss function and optimizer as Lab 1:\n",
    "- **Loss**: `BCEWithLogitsLoss` (Binary Cross Entropy with built-in sigmoid)\n",
    "- **Optimizer**: SGD with learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"Loss function: {loss_fn}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Model\n",
    "\n",
    "Let's train our non-linear model for 1000 epochs. The training loop follows these 5 steps:\n",
    "\n",
    "1. Zero gradients\n",
    "2. Forward pass\n",
    "3. Calculate loss\n",
    "4. Backward pass (backpropagation)\n",
    "5. Optimizer step\n",
    "\n",
    "**Watch the accuracy climb!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "torch.manual_seed(42)\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_2.train()\n",
    "    \n",
    "    # 1. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    y_logits = model_2(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits))  # logits -> probs -> labels\n",
    "    \n",
    "    # 3. Calculate loss and accuracy\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 4. Backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_2(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* If you notice the `Training` and `Testing` accuracy is not improved significantly, try to **Restart the Kernel** and run the notebook again. This is because the model weights are initialized randomly, and sometimes the random initialization can lead to poor convergence. Additionally, if you run the training cell multiple times without restarting, the model continues training from its current state rather than starting fresh, which can cause unexpected results. Restarting ensures a clean state with fresh random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Model\n",
    "\n",
    "Look at that improvement! The accuracy jumped significantly from ~50% (Lab 1)!\n",
    "\n",
    "Let's make final predictions and calculate the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_2.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = torch.round(torch.sigmoid(model_2(X_test))).squeeze()\n",
    "\n",
    "# Calculate final accuracy\n",
    "final_acc = accuracy_fn(y_true=y_test, y_pred=y_preds)\n",
    "print(f\"Final Test Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "# Show some predictions\n",
    "print(f\"\\nFirst 10 predictions: {y_preds[:10].int().tolist()}\")\n",
    "print(f\"First 10 actual:      {y_test[:10].int().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing the Decision Boundary\n",
    "\n",
    "Now let's see the decision boundary our model learned. Unlike Lab 1's straight line, we should see a **curved boundary** that follows the circle pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundaries for training and test sets\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_2, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_2, X_test, y_test)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model learned a circular decision boundary!** This perfectly separates the inner and outer circles.\n",
    "\n",
    "The ReLU activation functions enabled the model to combine multiple \"bent\" lines into a curved shape that matches our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparing Linear vs Non-Linear Models\n",
    "\n",
    "Let's create both models and compare their decision boundaries side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear model (like Lab 1)\n",
    "class CircleModelLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "        # NO ReLU!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "# Train linear model\n",
    "torch.manual_seed(42)\n",
    "model_linear = CircleModelLinear()\n",
    "optimizer_linear = torch.optim.SGD(model_linear.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    model_linear.train()\n",
    "    y_logits = model_linear(X_train).squeeze()\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "    optimizer_linear.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_linear.step()\n",
    "\n",
    "# Calculate accuracies\n",
    "model_linear.eval()\n",
    "with torch.inference_mode():\n",
    "    linear_preds = torch.round(torch.sigmoid(model_linear(X_test))).squeeze()\n",
    "    linear_acc = accuracy_fn(y_test, linear_preds)\n",
    "\n",
    "print(f\"Linear Model Accuracy: {linear_acc:.2f}%\")\n",
    "print(f\"Non-Linear Model Accuracy: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare decision boundaries\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"Linear Model (No ReLU) - Acc: {linear_acc:.1f}%\")\n",
    "plot_decision_boundary(model_linear, X_test, y_test)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f\"Non-Linear Model (With ReLU) - Acc: {final_acc:.1f}%\")\n",
    "plot_decision_boundary(model_2, X_test, y_test)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "Congratulations on completing Lab 2! You've successfully solved the non-linear classification problem that stumped our linear model in Lab 1.\n",
    "\n",
    "### What We Achieved\n",
    "\n",
    "In this lab, we transformed a failing model into a working one by understanding a fundamental concept in deep learning: **non-linearity is essential for learning complex patterns**.\n",
    "\n",
    "**The Problem**: Our circle data couldn't be separated by a straight line, and stacking linear layers only produces more linear transformations.\n",
    "\n",
    "**The Solution**: By adding ReLU activation functions between layers, we enabled our neural network to learn curved decision boundaries that match the circular pattern in our data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Non-linear activation functions are essential** for learning complex patterns\n",
    "2. **ReLU is simple but powerful**: `max(0, x)` enables universal function approximation\n",
    "3. **Model architecture matters**: Adding more linear layers doesn't help without non-linearity\n",
    "4. **Visual inspection helps**: Decision boundary plots reveal what the model actually learned\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Lab 3**, we'll extend our knowledge to **multiclass classification** — problems with more than 2 classes. We'll use softmax activation and cross-entropy loss to handle multiple categories!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
